{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.12.12"},"colab":{"provenance":[],"gpuType":"T4","include_colab_link":true},"accelerator":"GPU","kaggle":{"accelerator":"none","dataSources":[{"sourceId":1905968,"sourceType":"datasetVersion","datasetId":1136210}],"dockerImageVersionId":31259,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":5,"nbformat":4,"cells":[{"id":"Jf47hMHlYuaF","cell_type":"markdown","source":"# â¤ï¸ Myocardial Infarction (MI) Detection from 12-Lead ECG (PTB-XL Dataset)\n\nThis notebook implements an automated framework for classifying ECG signals using **classical Machine Learning** and **Deep Learning** techniques.\n\nWe compare two distinct approaches for **Binary Classification** (Normal vs. Abnormal):\n1.  **Feature-Based Approach (ML):** Extracting morphological features using Median Beats and PCA, classified via Logistic Regression and Random Forest.\n2.  **End-to-End Approach (DL):** Using Raw Signal data fed directly into a 1D Convolutional Neural Network (CNN).\n\n### ðŸ”¹ Key Methodologies\n* **Dataset:** PTB-XL (500 Hz), utilizing **all 12 Leads**.\n* **Preprocessing:** Bandpass filtering (0.5-50 Hz) & R-Peak detection on Lead II.\n* **Feature Engineering (Phase 2):**\n    * **Median Beat Construction:** Robust \"Territory Logic\" (dynamic windowing) to handle Tachycardia/Bradycardia.\n    * **Dimensionality Reduction:** Universal PCA to extract **60 standardized features** (5 components per lead).\n* **Models:**\n    * **Logistic Regression:** Baseline linear classifier on PCA features.\n    * **Random Forest:** Non-linear ensemble classifier on PCA features.\n    * **1D-CNN:** Deep learning model trained on **Raw 12-lead Time-Series** (5000 samples).\n\n### ðŸ”¹ Goal\nTo evaluate if engineered morphological features (PCA) can compete with the raw signal learning capabilities of Convolutional Neural Networks for detecting cardiac abnormalities.","metadata":{"id":"Jf47hMHlYuaF"}},{"id":"byAtJYFe97mU","cell_type":"markdown","source":"## Google Colab Requirements","metadata":{"id":"byAtJYFe97mU"}},{"id":"9c99f151","cell_type":"code","source":"%pip install wfdb --no-deps","metadata":{"id":"9c99f151","trusted":true},"outputs":[],"execution_count":null},{"id":"DGwBVm-h-BUk","cell_type":"markdown","source":"## Import and Configuration","metadata":{"id":"DGwBVm-h-BUk"}},{"id":"5xit-gITYuaI","cell_type":"code","source":"# --- Standard Library ---\nimport os\nimport json\nimport random\nfrom pathlib import Path\n\n# --- Data Manipulation & Math ---\nimport numpy as np\nimport pandas as pd\nfrom tqdm import tqdm\nimport joblib\n\n# --- Visualization ---\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# --- Signal Processing & Domain Specific ---\nimport wfdb\nfrom scipy.signal import find_peaks\nfrom scipy.stats import skew, kurtosis\n\n# --- Scikit-Learn (Machine Learning) ---\nfrom sklearn.decomposition import PCA\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.utils.class_weight import compute_class_weight\nfrom sklearn.metrics import (\n    accuracy_score, precision_score, recall_score, f1_score,\n    confusion_matrix, classification_report, ConfusionMatrixDisplay\n)\n\n# --- TensorFlow / Keras (Deep Learning) ---\nimport tensorflow as tf\nfrom keras import mixed_precision\nfrom keras import Sequential, backend as K\nfrom keras.models import Model, load_model\nfrom keras.optimizers import Adam\nfrom keras.callbacks import ReduceLROnPlateau, EarlyStopping, ModelCheckpoint\nfrom keras.layers import (\n    Input, Conv1D, MaxPooling1D, Dense, Dropout, SpatialDropout1D,\n    Activation,BatchNormalization, Add, GlobalAveragePooling1D, Concatenate,\n    Lambda, Reshape, UpSampling1D, Conv1DTranspose, Layer, Cropping1D\n)\n\n# --- Configuration & Constants ---\nINPUT_DIR = \"/kaggle/input/\"\nOUTPUT_DIR = \"/kaggle/working/\"\nDATASET_PATH = os.path.join(INPUT_DIR, \"ptb-xl-dataset/ptb-xl-a-large-publicly-available-electrocardiography-dataset-1.0.1\")\nMETA_FILE_PTB = os.path.join(DATASET_PATH, \"ptbxl_database.csv\")\nMETA_FILE_SCP = os.path.join(DATASET_PATH, \"scp_statements.csv\")\n\nSEED = 42\nSAMPLE_RATE = 500\nTEST_SIZE = 0.2\nMAX_RECORDS = 3000\n\nCNN_EPOCHS = 50\nBATCH_SIZE = 32\n\n# Set Seeds\nrandom.seed(SEED)\nnp.random.seed(SEED)\ntf.random.set_seed(SEED)","metadata":{"id":"5xit-gITYuaI","trusted":true},"outputs":[],"execution_count":null},{"id":"wjCdsxWDYuaJ","cell_type":"markdown","source":"## Helper Functions","metadata":{"id":"wjCdsxWDYuaJ"}},{"id":"W1NlY69hYuaJ","cell_type":"code","source":"def safe_eval_scp(scp_code):\n    try: return eval(scp_code) if isinstance(scp_code, str) else scp_code\n    except: return {}\n\ndef ensure_len(sig, expected):\n    # sig shape: (time, leads) or (time,)\n    length = len(sig)\n    if length == expected:\n        return sig\n\n    if length > expected:\n        return sig[:expected]\n\n    # Padding logic for 2D (12 leads) or 1D (single lead)\n    diff = expected - length\n    if sig.ndim == 1:\n        pad = np.zeros(diff)\n        return np.concatenate([sig, pad])\n    else:\n        # Assume shape (time, leads), pad along axis 0 (time)\n        leads = sig.shape[1]\n        pad = np.zeros((diff, leads))\n        return np.concatenate([sig, pad], axis=0)\n\ndef normalize_per_record(X):\n    # X shape: (n_samples, signal_len, n_leads)\n    # Mean and Std along the signal_len dimension (axis 1)\n    m = X.mean(axis=1, keepdims=True)\n    s = X.std(axis=1, keepdims=True) + 1e-8\n    return (X - m) / s\n\ndef print_result(res):\n    print(f\"\\nModel: {res['model']}\")\n    print(f\"Accuracy: {res['accuracy']:.2%}\")\n    print(f\"Precision: {res['precision']:.2%}\")\n    print(f\"Recall: {res['recall']:.2%}\")\n    print(f\"F1 Score: {res['f1 score']:.2%}\")","metadata":{"id":"W1NlY69hYuaJ","trusted":true},"outputs":[],"execution_count":null},{"id":"X1X2asfGYuaJ","cell_type":"markdown","source":"## Load Metadata","metadata":{"id":"X1X2asfGYuaJ"}},{"id":"nXTbsJvpqS4z","cell_type":"code","source":"def load_metadata(max_records=MAX_RECORDS):\n    # Load raw dataframes\n    ptb_df = pd.read_csv(META_FILE_PTB)\n    scp_df = pd.read_csv(META_FILE_SCP, index_col=0)\n\n    # Convert scp_codes from string to dict\n    ptb_df[\"scp_codes\"] = ptb_df[\"scp_codes\"].apply(safe_eval_scp)\n\n    # Drop records without a filename\n    ptb_df = ptb_df[ptb_df[\"filename_hr\"].notna()].reset_index(drop=True)\n\n    # --- Step 1: Build the Dictionary Map (Code -> Superclass) ---\n    # We only keep codes that have a valid diagnostic_class (MI, STTC, CD, HYP, NORM)\n    scp_df = scp_df[scp_df[\"diagnostic_class\"].notna()]\n    code_map = scp_df[\"diagnostic_class\"].to_dict()\n\n    # --- Step 2: Define the Labeling Function with Hierarchy ---\n    def get_label(scp_dict):\n        # Get all superclasses present for this patient\n        classes = set()\n        for code in scp_dict.keys():\n            if code in code_map:\n                classes.add(code_map[code])\n\n        # Check Priority: MI > STTC > CD > HYP > NORM\n        if \"MI\" in classes: return 1      # MI\n        if \"STTC\" in classes: return 2    # STTC\n        if \"CD\" in classes: return 3      # CD\n        if \"HYP\" in classes: return 4     # HYP\n        if \"NORM\" in classes: return 0    # NORM\n\n        return -1 # Unknown/Other (will be dropped)\n\n    # --- Step 3: Apply Labels ---\n    ptb_df[\"label_multiclass\"] = ptb_df[\"scp_codes\"].apply(get_label)\n\n    # Drop \"Other/Unknown\" (-1)\n    ptb_df = ptb_df[ptb_df[\"label_multiclass\"] != -1].reset_index(drop=True)\n\n    # Create Binary Label: 0=NORM, 1=Abnormal (MI, STTC, CD, HYP)\n    ptb_df[\"label_binary\"] = ptb_df[\"label_multiclass\"].apply(lambda x: 0 if x == 0 else 1)\n\n    # --- Step 4: Stratified Sampling (Optional) ---\n    if max_records is not None and max_records < len(ptb_df):\n        from sklearn.model_selection import train_test_split\n        # Stratify by multiclass label to keep distribution balanced\n        ptb_df, _ = train_test_split(\n            ptb_df,\n            train_size=max_records,\n            stratify=ptb_df[\"label_multiclass\"],\n            random_state=SEED,\n        )\n        ptb_df = ptb_df.reset_index(drop=True)\n\n    return ptb_df\n\nptb_df = load_metadata()\nprint(\"Total records used:\", len(ptb_df))\nprint(\"\\nMulticlass Counts (0=NORM, 1=MI, 2=STTC, 3=CD, 4=HYP):\")\nprint(ptb_df[\"label_multiclass\"].value_counts().sort_index())\nprint(\"\\nBinary Counts (0=Normal, 1=Abnormal):\")\nprint(ptb_df[\"label_binary\"].value_counts())\nptb_df[[\"ecg_id\", \"scp_codes\", \"label_multiclass\", \"label_binary\"]]","metadata":{"id":"nXTbsJvpqS4z","trusted":true},"outputs":[],"execution_count":null},{"id":"EH1Bu5CoYuaJ","cell_type":"markdown","source":"## Load One Signal and Smoke Test","metadata":{"id":"EH1Bu5CoYuaJ"}},{"id":"tuLAXH2nYuaJ","cell_type":"code","source":"def load_signal_single(path_record):\n    \"\"\"\n    Loads all 12 leads of the ECG signal.\n    Returns: numpy array of shape (time_steps, 12)\n    \"\"\"\n    try:\n        recp = os.path.join(DATASET_PATH, path_record)\n        # wfdb.rdsamp returns (signal, fields). We only need signal.\n        sig, fields = wfdb.rdsamp(recp)\n        return sig.astype(np.float32)\n    except:\n        return None\n\ndef smoke_test(df, count=1):\n    print(f\"Showing first {count} records (12-Lead view)...\")\n\n    lead_names = ['I', 'II', 'III', 'AVR', 'AVL', 'AVF', 'V1', 'V2', 'V3', 'V4', 'V5', 'V6']\n\n    for row in df.itertuples():\n        sig = load_signal_single(row.filename_hr)\n        if sig is None:\n            continue\n\n        # Ensure length (now handles 12 leads)\n        sig = ensure_len(sig, SAMPLE_RATE * 10)\n\n        # Plot all 12 leads stacked\n        fig, axes = plt.subplots(12, 1, figsize=(10, 15), sharex=True)\n\n        for i in range(12):\n            ax = axes[i]\n            ax.plot(sig[:, i], color='black', linewidth=0.8)\n            ax.set_ylabel(lead_names[i], rotation=0, labelpad=20, fontsize=10, fontweight='bold')\n            ax.grid(True, linestyle='--', alpha=0.5)\n            if i < 11:\n                ax.tick_params(labelbottom=False)\n\n        plt.suptitle(f\"Record: {row.filename_hr}\\nBinary: {row.label_binary} | Multi: {row.label_multiclass}\", y=1.02)\n        plt.xlabel(\"Time (samples)\")\n        plt.tight_layout()\n        plt.show()\n\n        count -= 1\n        if count == 0:\n            return\n\nsmoke_test(df=ptb_df)","metadata":{"id":"tuLAXH2nYuaJ","trusted":true},"outputs":[],"execution_count":null},{"id":"cNH48EgoYuaK","cell_type":"markdown","source":"## Build Raw Dataset","metadata":{"id":"cNH48EgoYuaK"}},{"id":"Tm6nRzRd-ZnS","cell_type":"code","source":"def build_raw_dataset(df):\n    X, y_bin, y_multi, rec_ids = [], [], [], []\n\n    for r in tqdm(df.itertuples(), desc=\"Loading ECG signals\"):\n        # Load the full 12-lead signal\n        sig = load_signal_single(r.filename_hr)\n        if sig is None:\n            continue\n\n        # Ensure length is exactly 5000 samples (handles 12 leads automatically)\n        sig = ensure_len(sig, SAMPLE_RATE * 10)\n\n        X.append(sig)\n        y_bin.append(r.label_binary)\n        y_multi.append(r.label_multiclass)\n        rec_ids.append(r.filename_hr)\n\n    # Stack to create 3D Array: (Samples, Time, Leads)\n    X = np.stack(X)\n    y_bin = np.array(y_bin)\n    y_multi = np.array(y_multi)\n\n    return X, y_bin, y_multi, rec_ids\n\n# --- Execute the Build ---\nX, y_bin, y_multi, rec_ids = build_raw_dataset(ptb_df)\n\n# --- Normalize ---\nX = normalize_per_record(X)\n\nprint(\"\\nDataset Built!\")\nprint(\"X shape:\", X.shape)\nprint(\"y_bin shape:\", y_bin.shape)\nprint(\"y_multi shape:\", y_multi.shape)","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Tm6nRzRd-ZnS","outputId":"48176de1-a7b9-45c9-9c54-2fb7cf2b300e","trusted":true},"outputs":[],"execution_count":null},{"id":"FCTcjFr4YuaK","cell_type":"markdown","source":"## Feature Extraction","metadata":{"id":"FCTcjFr4YuaK"}},{"id":"OgN19ARjYuaK","cell_type":"code","source":"WINDOW_LEN = 300\nPRE_R = 100\nPOST_R = 200\nSAFETY_MARGIN = 10\n\nN_PCA_COMPONENTS = 5\n\n\ndef get_median_beats(sig):\n    \"\"\"\n    Extracts the median beat from a 12-lead signal.\n    Input: sig (5000, 12)\n    Output: median_beat (12, 300) -> Transposed for PCA consistency\n    \"\"\"\n    # 1. Detect Peaks on Lead II (Index 1)\n    peaks, _ = find_peaks(sig[:, 1], distance=150, height=0.1)\n\n    # If no peaks, return zeros (12 leads x 300 samples)\n    if len(peaks) == 0:\n        return np.zeros((12, WINDOW_LEN), dtype=np.float32)\n\n    beats = []\n\n    for i, r_curr in enumerate(peaks):\n        # --- A. Define Ideal Territory ---\n        ideal_start = r_curr - PRE_R\n        ideal_end = r_curr + POST_R\n\n        # --- B. Define Safety Limits ---\n        if i > 0:\n            limit_start = peaks[i-1] + POST_R + SAFETY_MARGIN\n        else:\n            limit_start = 0\n\n        if i < len(peaks) - 1:\n            limit_end = peaks[i+1] - PRE_R - SAFETY_MARGIN\n        else:\n            limit_end = sig.shape[0]\n\n        # --- C. Truncate ---\n        actual_start = max(ideal_start, limit_start)\n        actual_end = min(ideal_end, limit_end)\n\n        actual_start = max(0, actual_start)\n        actual_end = min(sig.shape[0], actual_end)\n\n        # --- D. The Canvas Method ---\n        canvas = np.zeros((WINDOW_LEN, 12), dtype=np.float32)\n        canvas_start_idx = PRE_R - (r_curr - actual_start)\n\n        chunk = sig[actual_start:actual_end]\n        chunk_len = len(chunk)\n\n        if chunk_len == 0 or canvas_start_idx < 0 or (canvas_start_idx + chunk_len) > WINDOW_LEN:\n            continue\n\n        canvas[canvas_start_idx : canvas_start_idx + chunk_len] = chunk\n        beats.append(canvas)\n\n    # --- E. Compute Median ---\n    if len(beats) == 0:\n        return np.zeros((12, WINDOW_LEN), dtype=np.float32)\n\n    beats = np.stack(beats) # Shape: (N_beats, 300, 12)\n    median_beat = np.median(beats, axis=0) # Shape: (300, 12)\n\n    # Transpose to return (12, 300)\n    return median_beat.T\n\ndef train_pca_model(X):\n    \"\"\"\n    Trains PCA on training set.\n    Input: X (N, 5000, 12)\n    Output: Trained PCA object\n    \"\"\"\n    print(\"Preparing data for PCA training...\")\n    all_beats = []\n\n    for x in tqdm(X, desc=\"Extracting Training Beats\"):\n        mb = get_median_beats(x) # Returns (12, 300)\n        all_beats.append(mb)     # List of (12, 300) arrays\n\n    # Stack: (N_patients * 12, 300)\n    X_for_pca = np.vstack(all_beats)\n\n    print(f\"Training PCA on {X_for_pca.shape[0]} individual lead waveforms...\")\n    pca = PCA(n_components=N_PCA_COMPONENTS)\n    pca.fit(X_for_pca)\n\n    print(f\"PCA Variance Explained: {np.sum(pca.explained_variance_ratio_):.2%}\")\n    return pca\n\ndef extract_pca_features(X, pca_model):\n    \"\"\"\n    Transforms signals into features.\n    Input: X (N, 5000, 12)\n    Output: features (N, 60)\n    \"\"\"\n    features = []\n\n    for x in tqdm(X, desc=\"Transforming Features\"):\n        mb = get_median_beats(x) # (12, 300)\n\n        # Project: Input (12, 300) -> Output (12, 5)\n        coeffs = pca_model.transform(mb)\n\n        # Flatten: 12 * 5 = 60 features\n        features.append(coeffs.flatten())\n\n    return np.array(features)","metadata":{"id":"OgN19ARjYuaK","trusted":true},"outputs":[],"execution_count":null},{"id":"xR1b0QMwYuaK","cell_type":"markdown","source":"## Train Test Split and Features Extraction","metadata":{"id":"xR1b0QMwYuaK"}},{"id":"0xQ1q4S6YuaK","cell_type":"code","source":"# 1. Stratified Split (Raw Data)\nX_train, X_test, y_train_bin, y_test_bin, y_train_multi, y_test_multi = train_test_split(\n    X,\n    y_bin,\n    y_multi,\n    test_size=TEST_SIZE,\n    stratify=y_multi,\n    random_state=SEED\n)\n\nprint(f\"Split Sizes: Train={len(X_train)}, Test={len(X_test)}\")\n\n# 2. Train PCA (On Training Set Only)\npca = train_pca_model(X_train)\n\n# 3. Extract Features\nprint(\"\\nExtracting features for Training Set...\")\nXf_train = extract_pca_features(X_train, pca)\n\nprint(\"\\nExtracting features for Test Set...\")\nXf_test = extract_pca_features(X_test, pca)\n\n# 4. Scale Features\nscaler = StandardScaler()\nXf_train = scaler.fit_transform(Xf_train)\nXf_test = scaler.transform(Xf_test)","metadata":{"id":"0xQ1q4S6YuaK","trusted":true},"outputs":[],"execution_count":null},{"id":"N3ra2ieaAqhz","cell_type":"markdown","source":"## Features Display","metadata":{"id":"N3ra2ieaAqhz"}},{"id":"PGDKcsfnAte3","cell_type":"code","source":"# --- Flexible Feature Display Function ---\ndef show_feature_table(features, lead_idx=None):\n    \"\"\"\n    Creates a DataFrame for the features and displays the head.\n    lead_idx: Integer (0-11) to filter for a specific lead.\n              If None, displays all 60 features.\n    \"\"\"\n    # 1. Generate Column Names\n    lead_names = ['I', 'II', 'III', 'AVR', 'AVL', 'AVF', 'V1', 'V2', 'V3', 'V4', 'V5', 'V6']\n    col_names = [f\"{lead}_C{i+1}\" for lead in lead_names for i in range(N_PCA_COMPONENTS)]\n\n    # 2. Create Full DataFrame\n    df = pd.DataFrame(features, columns=col_names)\n\n    # 3. Filter or Show All\n    if lead_idx is not None:\n        if 0 <= lead_idx < 12:\n            target_lead = lead_names[lead_idx]\n            # Select columns containing the lead name (e.g., \"II_C1\", \"II_C2\"...)\n            # We use specific slicing math: start = idx*5, end = start+5\n            start_col = lead_idx * N_PCA_COMPONENTS\n            end_col = start_col + N_PCA_COMPONENTS\n\n            print(f\"\\n--- Features for Lead {target_lead} (Index {lead_idx}) ---\")\n            display(df.iloc[:, start_col:end_col].head())\n        else:\n            print(f\"Error: Lead index must be 0-11.\")\n    else:\n        print(\"\\n--- All 60 Features (12 Leads x 5 Components) ---\")\n        display(df.head())\n\n# --- Usage Examples ---\n# Example 1: Show Lead II (Index 1) - Often the most important lead\nshow_feature_table(Xf_train, lead_idx=1)\n\n# Example 2: Show All Features\n# show_feature_table(Xf_train)","metadata":{"id":"PGDKcsfnAte3","trusted":true},"outputs":[],"execution_count":null},{"id":"9Z_mctMpA37S","cell_type":"markdown","source":"## Universal Evaluator","metadata":{"id":"9Z_mctMpA37S"}},{"id":"gaW5-cuCBEBv","cell_type":"code","source":"def evaluate_model(model_name, y_true, y_pred):\n    \"\"\"\n    Calculates metrics and displays confusion matrix.\n    Returns a dictionary of metrics.\n    \"\"\"\n    print(f\"\\n{'='*40}\")\n    print(f\"EVALUATION: {model_name}\")\n    print(f\"{'='*40}\")\n\n    # 1. Calculate Metrics\n    acc = accuracy_score(y_true, y_pred)\n    prec = precision_score(y_true, y_pred, zero_division=0)\n    rec = recall_score(y_true, y_pred, zero_division=0)\n    f1 = f1_score(y_true, y_pred, zero_division=0)\n\n    # 2. Print Report\n    print(f\"Accuracy:  {acc:.4f}\")\n    print(f\"Precision: {prec:.4f}\")\n    print(f\"Recall:    {rec:.4f}\")\n    print(f\"F1 Score:  {f1:.4f}\")\n    print(\"-\" * 20)\n    print(\"Classification Report:\")\n    print(classification_report(y_true, y_pred, target_names=['Normal', 'Abnormal']))\n\n    # 3. Plot Confusion Matrix\n    cm = confusion_matrix(y_true, y_pred)\n    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=['Normal', 'Abnormal'])\n\n    fig, ax = plt.subplots(figsize=(5, 5))\n    disp.plot(cmap='Blues', ax=ax, colorbar=False)\n    ax.set_title(f\"{model_name} Confusion Matrix\")\n    plt.show()\n\n    # 4. Return Object\n    return {\n        \"model\": model_name,\n        \"accuracy\": acc,\n        \"precision\": prec,\n        \"recall\": rec,\n        \"f1 score\": f1\n    }\n","metadata":{"id":"gaW5-cuCBEBv","trusted":true},"outputs":[],"execution_count":null},{"id":"GD4DLoAzYuaK","cell_type":"markdown","source":"## Machine Learning Models Training","metadata":{"id":"GD4DLoAzYuaK"}},{"id":"X6EAxSr_YuaK","cell_type":"code","source":"# Train Logistic Regression Model\nlr_model = LogisticRegression(max_iter=1000, random_state=SEED)\nprint(\"Training Logistic Regression on PCA Features...\")\nlr_model.fit(Xf_train, y_train_bin)\n\n# Train Random Forest Model\nrf_model = RandomForestClassifier(n_estimators=200, random_state=SEED)\nprint(\"Training Random Forest on PCA Features...\")\nrf_model.fit(Xf_train, y_train_bin)","metadata":{"id":"X6EAxSr_YuaK","trusted":true},"outputs":[],"execution_count":null},{"id":"8bRhfMMgYuaK","cell_type":"markdown","source":"## Machine Learning Models Evaluation","metadata":{"id":"8bRhfMMgYuaK"}},{"id":"eGMcqjDtYuaK","cell_type":"code","source":"# Train Logistic Regression Model\ny_pred_lr = lr_model.predict(Xf_test)\nres_lr = evaluate_model(\"Logistic Regression (PCA)\", y_test_bin, y_pred_lr)\njoblib.dump(lr_model, os.path.join(OUTPUT_DIR, 'lr_model.pkl'))\n\n# Train Random Forest Model\ny_pred_rf = rf_model.predict(Xf_test)\nres_rf = evaluate_model(\"Random Forest (PCA)\", y_test_bin, y_pred_rf)\njoblib.dump(rf_model, os.path.join(OUTPUT_DIR, 'rf_model.pkl'))","metadata":{"id":"eGMcqjDtYuaK","trusted":true},"outputs":[],"execution_count":null},{"id":"JqK2SFFUYuaK","cell_type":"markdown","source":"## CNN Model Building and Training (Binary Classification)","metadata":{"id":"JqK2SFFUYuaK"}},{"id":"ScjmD1UpYuaK","cell_type":"code","source":"# 1. Define CNN Architecture\ndef build_cnn(input_shape):\n    model = Sequential()\n    model.add(Input(shape=input_shape))\n\n    # --- Block 1: The \"Spike Detector\" (Low-level features) ---\n    model.add(Conv1D(filters=32, kernel_size=10, padding='same'))\n    model.add(BatchNormalization())\n    model.add(Activation('relu'))\n    model.add(MaxPooling1D(pool_size=2))\n\n    # --- Block 2: The \"Wave Detector\" (Mid-level features) ---\n    model.add(Conv1D(filters=64, kernel_size=10, padding='same'))\n    model.add(BatchNormalization())\n    model.add(Activation('relu'))\n    model.add(MaxPooling1D(pool_size=2))\n\n    # --- Block 3: The \"Complex Shape Detector\" (High-level features) ---\n    # This layer sees the relationship between P, QRS, and T\n    model.add(Conv1D(filters=128, kernel_size=10, padding='same'))\n    model.add(BatchNormalization())\n    model.add(Activation('relu'))\n    model.add(MaxPooling1D(pool_size=2))\n\n    # --- Block 4: Optional Deep Feature Extraction ---\n    model.add(Conv1D(filters=256, kernel_size=10, padding='same'))\n    model.add(BatchNormalization())\n    model.add(Activation('relu'))\n    # Global Average Pooling replaces Flatten (Reduces parameters, prevents overfitting)\n    model.add(GlobalAveragePooling1D())\n\n    # --- Classifier Head ---\n    model.add(Dense(128, activation='relu'))\n    model.add(Dropout(0.5)) # Standard dropout to force robustness\n    model.add(Dense(64, activation='relu'))\n    model.add(Dropout(0.3))\n\n    model.add(Dense(1, activation='sigmoid')) # Binary Output\n\n    # Compile\n    optimizer = Adam(learning_rate=0.001)\n    model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy'])\n    return model\n\n# 3. Build\nprint(f\"\\n{'='*40}\")\nprint(\"TRAINING CNN\")\nprint(f\"{'='*40}\")\n\ncnn = build_cnn((5000, 12))\n\ncallbacks_cnn = [\n    EarlyStopping(monitor='val_loss', patience=8, restore_best_weights=True, verbose=1),\n    ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=3, min_lr=0.00001, verbose=1)\n]\n\n# 4. Train\nhistory_cnn = cnn.fit(\n    X_train, y_train_bin,\n    validation_data=(X_test, y_test_bin),\n    epochs=CNN_EPOCHS,\n    batch_size=BATCH_SIZE,\n    callbacks=callbacks_cnn,\n    verbose=1\n)\n\n#5. Save\ncnn.save(os.path.join(OUTPUT_DIR, 'cnn_model_raw.h5'))","metadata":{"id":"ScjmD1UpYuaK","trusted":true},"outputs":[],"execution_count":null},{"id":"2fLHl6yGDSEd","cell_type":"markdown","source":"##  CNN Model Evaluation (Binary Classification)","metadata":{"id":"2fLHl6yGDSEd"}},{"id":"06Dkk02LDXRQ","cell_type":"code","source":"# Predict\nprint(f\"\\n{'='*50}\")\nprint(\"CNN EVALUATION\")\nprint(f\"{'='*50}\")\n\nprint(\"\\nGenerating Predictions...\")\ny_pred_prob_cnn = cnn.predict(X_test)\ny_pred_bin_cnn = (y_pred_prob_cnn > 0.5).astype(int).flatten()\n\n# Evaluate\nprint(\"\\nFinal Evaluation Results:\")\nres_cnn = evaluate_model(\"CNN (Raw)\", y_test_bin, y_pred_bin_cnn)","metadata":{"id":"06Dkk02LDXRQ","trusted":true},"outputs":[],"execution_count":null},{"id":"MEXZJCVDEZPF","cell_type":"markdown","source":"## AG-ResVAE Model Building and Training (Binary Classification)","metadata":{"id":"MEXZJCVDEZPF"}},{"id":"qYHp0Sk3KfLY","cell_type":"code","source":"# ==============================================================================\n# 1. MODEL CONFIGURATION\n# ==============================================================================\nINPUT_SHAPE = (5000, 12)\nNUM_CLASSES = 1\nPOOL_SIZE = 2\nNUM_BLOCKS = 4\nFILTERS_START = 64\nLATENT_DIM = 128\n\nLEARNING_RATE = 0.001\nBATCH_SIZE = 16\nEPOCHS = 50\n\nWEIGHT_CLASSIFICATION = 1.0\nWEIGHT_RECONSTRUCTION = 0.5\nKL_LOSS_WEIGHT = 0.01\n\nIDX_SEPTAL = [6, 7]\nIDX_ANTERIOR = [8, 9]\nIDX_LATERAL = [0, 4, 10, 11]\nIDX_INFERIOR = [1, 2, 5]\nIDX_POSTERIOR = [3]\n\n# ==============================================================================\n# 2. HELPER BLOCKS\n# ==============================================================================\n\ndef residual_block(x, filters, kernel_size=7, stride=1):\n    \"\"\"Creates a Residual Block.\"\"\"\n\n    shortcut = x\n\n    # Path A\n    x = Conv1D(filters, kernel_size, strides=stride, padding='same')(x)\n    x = BatchNormalization()(x)\n    x = Activation('swish')(x)\n    x = SpatialDropout1D(0.3)(x)\n\n    x = Conv1D(filters, kernel_size, strides=1, padding='same')(x)\n    x = BatchNormalization()(x)\n\n    # Path B\n    if x.shape[-1] != shortcut.shape[-1] or stride != 1:\n        shortcut = Conv1D(filters, 1, strides=stride, padding='same')(shortcut)\n        shortcut = BatchNormalization()(shortcut)\n\n    x = Add()([x, shortcut])\n    x = Activation('swish')(x)\n    return x\n\nclass Sampling(Layer):\n    \"\"\"VAE Sampling Layer.\"\"\"\n    def call(self, inputs):\n        z_mean, z_log_var = inputs\n        batch = tf.shape(z_mean)[0]\n        dim = tf.shape(z_mean)[1]\n        epsilon = tf.keras.backend.random_normal(shape=(batch, dim))\n        return z_mean + tf.exp(0.5 * z_log_var) * epsilon\n\nclass KLLossLayer(Layer):\n    \"\"\"Calculates KL Divergence Loss safely.\"\"\"\n    def call(self, inputs):\n        z_mean, z_log_var = inputs\n        kl_loss = -0.5 * tf.reduce_mean(1 + z_log_var - tf.square(z_mean) - tf.exp(z_log_var))\n        self.add_loss(kl_loss * KL_LOSS_WEIGHT)\n        return inputs\n\n# ==============================================================================\n# 3. ARCHITECTURE BUILDER\n# ==============================================================================\n\ndef build_ag_resvae(input_shape=INPUT_SHAPE, latent_dim=LATENT_DIM):\n\n    # --- BLOCK 1: INPUT & TRIAGE ---\n    input_layer = Input(shape=input_shape, name=\"ecg_input\")\n\n    septal = Lambda(lambda x: tf.gather(x, IDX_SEPTAL, axis=2), name=\"Septal\")(input_layer)\n    anterior = Lambda(lambda x: tf.gather(x, IDX_ANTERIOR, axis=2), name=\"Anterior\")(input_layer)\n    lateral = Lambda(lambda x: tf.gather(x, IDX_LATERAL, axis=2), name=\"Lateral\")(input_layer)\n    inferior = Lambda(lambda x: tf.gather(x, IDX_INFERIOR, axis=2), name=\"Inferior\")(input_layer)\n    posterior = Lambda(lambda x: tf.gather(x, IDX_POSTERIOR, axis=2), name=\"Posterior\")(input_layer)\n\n    # --- BLOCK 2: ENCODER ---\n    branches = []\n    for region_name, branch_in in zip([\"Sept\", \"Ant\", \"Lat\", \"Inf\", \"Post\"],\n                                      [septal, anterior, lateral, inferior, posterior]):\n\n        # Initial Feature Extraction\n        x = Conv1D(FILTERS_START, 7, strides=POOL_SIZE, padding='same', name=f\"{region_name}_Conv1\")(branch_in)\n        x = BatchNormalization()(x)\n        x = Activation('swish')(x)\n\n        # Dynamic Residual Stacking\n        current_filters = FILTERS_START * 2\n        for i in range(NUM_BLOCKS):\n            x = residual_block(x, current_filters, stride=POOL_SIZE)\n            current_filters *= 2\n\n        x = GlobalAveragePooling1D(name=f\"{region_name}_GlobalPool\")(x)\n        branches.append(x)\n\n    # --- BLOCK 3: BOTTLENECK (Latent Vector creation) ---\n    merged = Concatenate(name=\"Anatomical_Fusion\")(branches)\n\n    z_mean = Dense(latent_dim, name=\"z_mean\")(merged)\n    z_log_var = Dense(latent_dim, name=\"z_log_var\")(merged)\n    _ = KLLossLayer()([z_mean, z_log_var])\n    z = Sampling(name=\"z_sampling\")([z_mean, z_log_var])\n\n    # --- BLOCK 4: HEAD A (Diagnosis) ---\n    c = Dense(256, activation='swish', kernel_initializer='he_normal')(z)\n    c = BatchNormalization()(c)\n    c = Dropout(0.4)(c)\n\n    c = Dense(128, activation='swish', kernel_initializer='he_normal')(c)\n    c = BatchNormalization()(c)\n    c = Dropout(0.4)(c)\n\n    classification_output = Dense(NUM_CLASSES, activation='sigmoid', name=\"classification\")(c)\n\n    # --- BLOCK 5: HEAD B (Reconstruction using Latent Vector) ---\n    # Calculate Expected Bottleneck Size (Rounding UP)\n    total_pools = 1 + NUM_BLOCKS\n    downsample_factor = POOL_SIZE ** total_pools\n    bottleneck_len = int(np.ceil(input_shape[0] / downsample_factor))\n\n    # Expand Latent Vector (z)\n    d = Dense(bottleneck_len * latent_dim, activation=\"swish\")(z)\n    d = Reshape((bottleneck_len, latent_dim))(d)\n\n    # Upsample (Reverse Loop)\n    filters_reverse = [2* FILTERS_START * (2**i) for i in range(NUM_BLOCKS)]\n    filters_reverse = filters_reverse[::-1]\n\n    for f in filters_reverse:\n        d = Conv1DTranspose(f, 7, strides=POOL_SIZE, padding='same', activation='swish')(d)\n\n    d = Conv1DTranspose(FILTERS_START, 7, strides=POOL_SIZE, padding='same', activation='swish')(d)\n\n    # SAFETY CROP\n    current_len = bottleneck_len * downsample_factor\n    crop_amount = current_len - input_shape[0]\n\n    if crop_amount > 0:\n        d = Cropping1D(cropping=(0, crop_amount), name=\"Safety_Crop\")(d)\n\n    reconstruction_output = Conv1D(input_shape[1], 7, padding='same', activation='linear', name=\"reconstruction\")(d)\n\n    # --- BLOCK 6: COMPILATION ---\n    model = Model(inputs=input_layer, outputs=[classification_output, reconstruction_output], name=\"AG_ResVAE\")\n\n    model.compile(\n        optimizer=Adam(learning_rate=LEARNING_RATE),\n        loss={\n            \"classification\": \"binary_crossentropy\",\n            \"reconstruction\": \"mse\"\n        },\n        loss_weights={\n            \"classification\": WEIGHT_CLASSIFICATION,\n            \"reconstruction\": WEIGHT_RECONSTRUCTION\n        },\n        metrics={\"classification\": \"accuracy\"}\n    )\n\n    return model\n\n# ==============================================================================\n# 4. EXECUTION PIPELINE\n# ==============================================================================\n\nprint(f\"{'='*50}\")\nprint(f\"AG-ResVAE TRAINING (OPTIMIZED)\")\nprint(f\"Input: {INPUT_SHAPE} | Latent Dim: {LATENT_DIM}\")\nprint(f\"Structure: {NUM_BLOCKS} Blocks | Start Filters: {FILTERS_START}\")\nprint(f\"{'='*50}\")\n\n# Build\nmodel_ag_resvae = build_ag_resvae()\n\n# Train\ncallbacks_ag_resvae = [\n    ReduceLROnPlateau(\n        monitor='val_classification_loss', factor=0.1, \n        patience=5, min_lr=1e-5, verbose=1, mode='min'\n    ),\n    EarlyStopping(\n        monitor='val_classification_loss', patience=12, \n        restore_best_weights=True, verbose=1, mode='min'\n    ),\n    ModelCheckpoint(\n        os.path.join(OUTPUT_DIR, \"best_ag_resvae.keras\"), \n        monitor='val_classification_accuracy', save_best_only=True, \n        verbose=1, mode='max'\n    )\n]\n\nsample_weights_array = np.ones(shape=(len(y_train_bin),))\nsample_weights_array[y_train_bin == 1] = 3.0\nsample_weights_recon = np.ones(shape=(len(y_train_bin),))\n\nprint(\"\\nStarting Training...\")\nhistory_ag = model_ag_resvae.fit(\n    x=X_train,\n    y={\n        \"classification\": y_train_bin,\n        \"reconstruction\": X_train\n    },\n    validation_data=(\n        X_test,\n        {\"classification\": y_test_bin, \"reconstruction\": X_test}\n    ),\n    epochs=EPOCHS,\n    batch_size=BATCH_SIZE,\n    callbacks=callbacks_ag_resvae,\n    sample_weight={\n        \"classification\": sample_weights_array,\n        \"reconstruction\": sample_weights_recon\n    },\n    verbose=1\n)","metadata":{"id":"qYHp0Sk3KfLY","trusted":true},"outputs":[],"execution_count":null},{"id":"0W6xTivHD82C","cell_type":"markdown","source":"## AG-ResVAE Model Evaluation (Binary Classification)","metadata":{"id":"0W6xTivHD82C"}},{"id":"3GTtUXAFD_9z","cell_type":"code","source":"# Predict\nprint(f\"\\n{'='*50}\")\nprint(\"AG-ResVAE EVALUATION\")\nprint(f\"{'='*50}\")\n\nprint(\"\\nGenerating Predictions...\")\nall_preds = model_ag_resvae.predict(X_test)\ny_pred_prob_ag = all_preds[0]\ny_pred_bin_ag = (y_pred_prob_ag > 0.5).astype(int).flatten()\n\n# Evaluate\nprint(\"\\nFinal Evaluation Results:\")\nres_ag_resvae2 = evaluate_model(\"AG-ResVAE (RAW)\", y_test_bin, y_pred_bin_ag)","metadata":{"id":"3GTtUXAFD_9z","trusted":true},"outputs":[],"execution_count":null},{"id":"Y6TcxCuUYuaL","cell_type":"markdown","source":"## Final Result Of All Models","metadata":{"id":"Y6TcxCuUYuaL"}},{"id":"e732549a-a366-46eb-90f1-f68c9fd94203","cell_type":"code","source":"# --- Standard Imports for Loading ---\nimport os\nimport joblib\nfrom keras.models import load_model\n\nprint(f\"{'='*50}\")\nprint(\"FINAL PIPELINE: LOADING MODELS & PREDICTING\")\nprint(f\"{'='*50}\\n\")\n\n# ==========================================\n# 1. Load Machine Learning Models (Scikit-Learn)\n# ==========================================\n\nprint(\"Loading ML Models...\")\nlr_loaded = joblib.load(os.path.join(OUTPUT_DIR, 'lr_model.pkl'))\nrf_loaded = joblib.load(os.path.join(OUTPUT_DIR, 'rf_model.pkl'))\n\n# ==========================================\n# 2. Load Deep Learning Models (Keras)\n# ==========================================\n\nprint(\"Loading DL Models...\")\n# Custom layer 'Sampling' and 'KLLossLayer' need to be passed for AG-ResVAE\ncnn_loaded = load_model(os.path.join(OUTPUT_DIR, 'cnn_model_raw.h5'))\nag_resvae_loaded = load_model(\n    os.path.join(OUTPUT_DIR, 'best_ag_resvae.keras'),\n    custom_objects={'Sampling': Sampling, 'KLLossLayer': KLLossLayer}\n)\n\nprint(\"All models loaded successfully!\\n\")\n\n# ==========================================\n# 3. Generate Predictions on Test Data\n# ==========================================\n\n# ML Predictions (using PCA features)\ny_pred_lr = lr_loaded.predict(Xf_test)\ny_pred_rf = rf_loaded.predict(Xf_test)\n\n# DL Predictions (using Raw signals)\ny_pred_prob_cnn = cnn_loaded.predict(X_test, verbose=0)\ny_pred_cnn = (y_pred_prob_cnn > 0.5).astype(int).flatten()\n\nag_resvae_preds = ag_resvae_loaded.predict(X_test, verbose=0)\ny_pred_prob_ag = ag_resvae_preds[0] # [0] is classification head\ny_pred_ag = (y_pred_prob_ag > 0.5).astype(int).flatten()\n\n# ==========================================\n# 4. Final Evaluation\n# ==========================================\n\nprint(\"\\n--- Final Test Set Evaluations ---\")\nfinal_lr = evaluate_model(\"Loaded Logistic Regression\", y_test_bin, y_pred_lr)\nfinal_rf = evaluate_model(\"Loaded Random Forest\", y_test_bin, y_pred_rf)\nfinal_cnn = evaluate_model(\"Loaded CNN\", y_test_bin, y_pred_cnn)\nfinal_ag = evaluate_model(\"Loaded AG-ResVAE\", y_test_bin, y_pred_ag)\n\n# Consolidate results into a final DataFrame\nfinal_results_df = pd.DataFrame([final_lr, final_rf, final_cnn, final_ag])\ndisplay(final_results_df.style.format({\n    'accuracy': '{:.2%}',\n    'precision': '{:.2%}',\n    'recall': '{:.2%}',\n    'f1 score': '{:.2%}'\n}))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"ca4943de","cell_type":"markdown","source":"# ðŸ“Œ Conclusion\n\nWe trained and compared **three models** for MI detection:\n\n### âœ” Logistic Regression (very fast, uses simple features)  \n### âœ” Random Forest (better than LR in many cases)  \n### âœ” 1D CNN (uses raw ECG, learns automatically)\n\nEvaluation metrics included:\n- Accuracy  \n- Precision  \n- Recall  \n- F1  \n- Confusion Matrix  ","metadata":{"id":"ca4943de"}}]}