{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.12.12"},"colab":{"provenance":[],"gpuType":"T4","include_colab_link":true},"accelerator":"GPU","kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":1905968,"sourceType":"datasetVersion","datasetId":1136210}],"dockerImageVersionId":31260,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":5,"nbformat":4,"cells":[{"id":"Jf47hMHlYuaF","cell_type":"markdown","source":"# â¤ï¸ Myocardial Infarction (MI) Detection from 12-Lead ECG (PTB-XL Dataset)\n\nThis notebook implements an automated framework for classifying ECG signals using **classical Machine Learning** and **Deep Learning** techniques.\n\nWe compare two distinct approaches for **Binary Classification** (Normal vs. Abnormal):\n1.  **Feature-Based Approach (ML):** Extracting morphological features using Median Beats and PCA, classified via Logistic Regression and Random Forest.\n2.  **End-to-End Approach (DL):** Using Raw Signal data fed directly into a 1D Convolutional Neural Network (CNN).\n\n### ðŸ”¹ Key Methodologies\n* **Dataset:** PTB-XL (500 Hz), utilizing **all 12 Leads**.\n* **Preprocessing:** Bandpass filtering (0.5-50 Hz) & R-Peak detection on Lead II.\n* **Feature Engineering (Phase 2):**\n    * **Median Beat Construction:** Robust \"Territory Logic\" (dynamic windowing) to handle Tachycardia/Bradycardia.\n    * **Dimensionality Reduction:** Universal PCA to extract **60 standardized features** (5 components per lead).\n* **Models:**\n    * **Logistic Regression:** Baseline linear classifier on PCA features.\n    * **Random Forest:** Non-linear ensemble classifier on PCA features.\n    * **1D-CNN:** Deep learning model trained on **Raw 12-lead Time-Series** (5000 samples).\n\n### ðŸ”¹ Goal\nTo evaluate if engineered morphological features (PCA) can compete with the raw signal learning capabilities of Convolutional Neural Networks for detecting cardiac abnormalities.","metadata":{"id":"Jf47hMHlYuaF"}},{"id":"byAtJYFe97mU","cell_type":"markdown","source":"## Requirements","metadata":{"id":"byAtJYFe97mU"}},{"id":"9c99f151","cell_type":"code","source":"%pip install wfdb --no-deps","metadata":{"id":"9c99f151","trusted":true,"execution":{"iopub.status.busy":"2026-01-26T12:17:50.142510Z","iopub.execute_input":"2026-01-26T12:17:50.143290Z","iopub.status.idle":"2026-01-26T12:17:52.578861Z","shell.execute_reply.started":"2026-01-26T12:17:50.143259Z","shell.execute_reply":"2026-01-26T12:17:52.577912Z"}},"outputs":[{"name":"stdout","text":"Collecting wfdb\n  Downloading wfdb-4.3.0-py3-none-any.whl.metadata (3.8 kB)\nDownloading wfdb-4.3.0-py3-none-any.whl (163 kB)\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m163.8/163.8 kB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hInstalling collected packages: wfdb\nSuccessfully installed wfdb-4.3.0\nNote: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}],"execution_count":1},{"id":"DGwBVm-h-BUk","cell_type":"markdown","source":"## Import and Configuration","metadata":{"id":"DGwBVm-h-BUk"}},{"id":"5xit-gITYuaI","cell_type":"code","source":"# --- Standard Library ---\nimport os\nimport json\nimport random\nfrom pathlib import Path\n\n# --- Data Manipulation & Math ---\nimport numpy as np\nimport pandas as pd\nfrom tqdm import tqdm\nimport joblib\n\n# --- Visualization ---\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# --- Signal Processing & Domain Specific ---\nimport wfdb\nfrom scipy.signal import find_peaks\nfrom scipy.stats import skew, kurtosis\n\n# --- Scikit-Learn (Machine Learning) ---\nfrom sklearn.decomposition import PCA\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.utils.class_weight import compute_class_weight\nfrom sklearn.metrics import (\n    accuracy_score, precision_score, recall_score, f1_score,\n    confusion_matrix, classification_report, ConfusionMatrixDisplay\n)\n\n# --- TensorFlow / Keras (Deep Learning) ---\nimport tensorflow as tf\nfrom keras import mixed_precision\nfrom keras import Sequential, backend as K\nfrom keras.models import Model, load_model\nfrom keras.optimizers import Adam\nfrom keras.callbacks import ReduceLROnPlateau, EarlyStopping, ModelCheckpoint\nfrom keras.layers import (\n    Input, Conv1D, MaxPooling1D, Dense, Dropout, SpatialDropout1D,\n    Activation,BatchNormalization, Add, GlobalAveragePooling1D, Concatenate,\n    Lambda, Reshape, UpSampling1D, Conv1DTranspose, Layer, Cropping1D\n)\n\n# --- Configuration & Constants ---\nINPUT_DIR = \"/kaggle/input/\"\nOUTPUT_DIR = \"/kaggle/working/\"\nDATASET_PATH = os.path.join(INPUT_DIR, \"ptb-xl-dataset/ptb-xl-a-large-publicly-available-electrocardiography-dataset-1.0.1\")\nMETA_FILE_PTB = os.path.join(DATASET_PATH, \"ptbxl_database.csv\")\nMETA_FILE_SCP = os.path.join(DATASET_PATH, \"scp_statements.csv\")\n\nSEED = 42\nSAMPLE_RATE = 500\nTEST_SIZE = 0.2\nMAX_RECORDS = 3000\n\nCNN_EPOCHS = 50\nBATCH_SIZE = 32\n\n# Set Seeds\nrandom.seed(SEED)\nnp.random.seed(SEED)\ntf.random.set_seed(SEED)","metadata":{"id":"5xit-gITYuaI","trusted":true,"execution":{"iopub.status.busy":"2026-01-26T12:17:52.580350Z","iopub.execute_input":"2026-01-26T12:17:52.580567Z","iopub.status.idle":"2026-01-26T12:18:07.897835Z","shell.execute_reply.started":"2026-01-26T12:17:52.580543Z","shell.execute_reply":"2026-01-26T12:18:07.897048Z"}},"outputs":[{"name":"stderr","text":"2026-01-26 12:17:56.154090: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1769429876.318987      55 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1769429876.366669      55 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\nW0000 00:00:1769429876.740946      55 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\nW0000 00:00:1769429876.740992      55 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\nW0000 00:00:1769429876.740995      55 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\nW0000 00:00:1769429876.740997      55 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n","output_type":"stream"}],"execution_count":2},{"id":"wjCdsxWDYuaJ","cell_type":"markdown","source":"## Helper Functions","metadata":{"id":"wjCdsxWDYuaJ"}},{"id":"W1NlY69hYuaJ","cell_type":"code","source":"def safe_eval_scp(scp_code):\n    try: return eval(scp_code) if isinstance(scp_code, str) else scp_code\n    except: return {}\n\ndef ensure_len(sig, expected):\n    # sig shape: (time, leads) or (time,)\n    length = len(sig)\n    if length == expected:\n        return sig\n\n    if length > expected:\n        return sig[:expected]\n\n    # Padding logic for 2D (12 leads) or 1D (single lead)\n    diff = expected - length\n    if sig.ndim == 1:\n        pad = np.zeros(diff)\n        return np.concatenate([sig, pad])\n    else:\n        # Assume shape (time, leads), pad along axis 0 (time)\n        leads = sig.shape[1]\n        pad = np.zeros((diff, leads))\n        return np.concatenate([sig, pad], axis=0)\n\ndef normalize_per_record(X):\n    # X shape: (n_samples, signal_len, n_leads)\n    # Mean and Std along the signal_len dimension (axis 1)\n    m = X.mean(axis=1, keepdims=True)\n    s = X.std(axis=1, keepdims=True) + 1e-8\n    return (X - m) / s\n\ndef print_result(res):\n    print(f\"\\nModel: {res['model']}\")\n    print(f\"Accuracy: {res['accuracy']:.2%}\")\n    print(f\"Precision: {res['precision']:.2%}\")\n    print(f\"Recall: {res['recall']:.2%}\")\n    print(f\"F1 Score: {res['f1 score']:.2%}\")","metadata":{"id":"W1NlY69hYuaJ","trusted":true,"execution":{"iopub.status.busy":"2026-01-26T12:18:07.898785Z","iopub.execute_input":"2026-01-26T12:18:07.899311Z","iopub.status.idle":"2026-01-26T12:18:07.905958Z","shell.execute_reply.started":"2026-01-26T12:18:07.899287Z","shell.execute_reply":"2026-01-26T12:18:07.905188Z"}},"outputs":[],"execution_count":3},{"id":"X1X2asfGYuaJ","cell_type":"markdown","source":"## Load Metadata","metadata":{"id":"X1X2asfGYuaJ"}},{"id":"nXTbsJvpqS4z","cell_type":"code","source":"def load_metadata(max_records=MAX_RECORDS):\n    # Load raw dataframes\n    ptb_df = pd.read_csv(META_FILE_PTB)\n    scp_df = pd.read_csv(META_FILE_SCP, index_col=0)\n\n    # Convert scp_codes from string to dict\n    ptb_df[\"scp_codes\"] = ptb_df[\"scp_codes\"].apply(safe_eval_scp)\n\n    # Drop records without a filename\n    ptb_df = ptb_df[ptb_df[\"filename_hr\"].notna()].reset_index(drop=True)\n\n    # --- Step 1: Build the Dictionary Map (Code -> Superclass) ---\n    # We only keep codes that have a valid diagnostic_class (MI, STTC, CD, HYP, NORM)\n    scp_df = scp_df[scp_df[\"diagnostic_class\"].notna()]\n    code_map = scp_df[\"diagnostic_class\"].to_dict()\n\n    # --- Step 2: Define the Labeling Function with Hierarchy ---\n    def get_label(scp_dict):\n        # Get all superclasses present for this patient\n        classes = set()\n        for code in scp_dict.keys():\n            if code in code_map:\n                classes.add(code_map[code])\n\n        # Check Priority: MI > STTC > CD > HYP > NORM\n        if \"MI\" in classes: return 1      # MI\n        if \"STTC\" in classes: return 2    # STTC\n        if \"CD\" in classes: return 3      # CD\n        if \"HYP\" in classes: return 4     # HYP\n        if \"NORM\" in classes: return 0    # NORM\n\n        return -1 # Unknown/Other (will be dropped)\n\n    # --- Step 3: Apply Labels ---\n    ptb_df[\"label_multiclass\"] = ptb_df[\"scp_codes\"].apply(get_label)\n\n    # Drop \"Other/Unknown\" (-1)\n    ptb_df = ptb_df[ptb_df[\"label_multiclass\"] != -1].reset_index(drop=True)\n\n    # Create Binary Label: 0=NORM, 1=Abnormal (MI, STTC, CD, HYP)\n    ptb_df[\"label_binary\"] = ptb_df[\"label_multiclass\"].apply(lambda x: 0 if x == 0 else 1)\n\n    # --- Step 4: Stratified Sampling (Optional) ---\n    if max_records is not None and max_records < len(ptb_df):\n        from sklearn.model_selection import train_test_split\n        # Stratify by multiclass label to keep distribution balanced\n        ptb_df, _ = train_test_split(\n            ptb_df,\n            train_size=max_records,\n            stratify=ptb_df[\"label_multiclass\"],\n            random_state=SEED,\n        )\n        ptb_df = ptb_df.reset_index(drop=True)\n\n    return ptb_df\n\nptb_df = load_metadata()\nprint(\"Total records used:\", len(ptb_df))\nprint(\"\\nMulticlass Counts (0=NORM, 1=MI, 2=STTC, 3=CD, 4=HYP):\")\nprint(ptb_df[\"label_multiclass\"].value_counts().sort_index())\nprint(\"\\nBinary Counts (0=Normal, 1=Abnormal):\")\nprint(ptb_df[\"label_binary\"].value_counts())\nptb_df[[\"ecg_id\", \"scp_codes\", \"label_multiclass\", \"label_binary\"]]","metadata":{"id":"nXTbsJvpqS4z","trusted":true,"execution":{"iopub.status.busy":"2026-01-26T12:18:07.907843Z","iopub.execute_input":"2026-01-26T12:18:07.908113Z","iopub.status.idle":"2026-01-26T12:18:08.674986Z","shell.execute_reply.started":"2026-01-26T12:18:07.908092Z","shell.execute_reply":"2026-01-26T12:18:08.674226Z"}},"outputs":[{"name":"stdout","text":"Total records used: 3000\n\nMulticlass Counts (0=NORM, 1=MI, 2=STTC, 3=CD, 4=HYP):\nlabel_multiclass\n0    1272\n1     768\n2     547\n3     338\n4      75\nName: count, dtype: int64\n\nBinary Counts (0=Normal, 1=Abnormal):\nlabel_binary\n1    1728\n0    1272\nName: count, dtype: int64\n","output_type":"stream"},{"execution_count":4,"output_type":"execute_result","data":{"text/plain":"      ecg_id                                          scp_codes  \\\n0       5181                        {'IRBBB': 100.0, 'SR': 0.0}   \n1      21559                         {'NORM': 100.0, 'SR': 0.0}   \n2      14982  {'LVH': 100.0, 'ISC_': 100.0, 'IVCD': 100.0, '...   \n3      20719                          {'NORM': 80.0, 'SR': 0.0}   \n4       8546                         {'LAFB': 100.0, 'SR': 0.0}   \n...      ...                                                ...   \n2995   21535          {'LVH': 50.0, 'ISC_': 100.0, 'AFIB': 0.0}   \n2996    9402  {'IMI': 100.0, 'ASMI': 100.0, '1AVB': 100.0, '...   \n2997   17394  {'LMI': 15.0, 'ASMI': 50.0, 'IRBBB': 100.0, 'A...   \n2998   18538  {'ASMI': 100.0, 'LAFB': 100.0, 'ABQRS': 0.0, '...   \n2999   14808  {'IMI': 100.0, 'ISCAL': 100.0, 'PVC': 100.0, '...   \n\n      label_multiclass  label_binary  \n0                    3             1  \n1                    0             0  \n2                    2             1  \n3                    0             0  \n4                    3             1  \n...                ...           ...  \n2995                 2             1  \n2996                 1             1  \n2997                 1             1  \n2998                 1             1  \n2999                 1             1  \n\n[3000 rows x 4 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>ecg_id</th>\n      <th>scp_codes</th>\n      <th>label_multiclass</th>\n      <th>label_binary</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>5181</td>\n      <td>{'IRBBB': 100.0, 'SR': 0.0}</td>\n      <td>3</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>21559</td>\n      <td>{'NORM': 100.0, 'SR': 0.0}</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>14982</td>\n      <td>{'LVH': 100.0, 'ISC_': 100.0, 'IVCD': 100.0, '...</td>\n      <td>2</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>20719</td>\n      <td>{'NORM': 80.0, 'SR': 0.0}</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>8546</td>\n      <td>{'LAFB': 100.0, 'SR': 0.0}</td>\n      <td>3</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>2995</th>\n      <td>21535</td>\n      <td>{'LVH': 50.0, 'ISC_': 100.0, 'AFIB': 0.0}</td>\n      <td>2</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2996</th>\n      <td>9402</td>\n      <td>{'IMI': 100.0, 'ASMI': 100.0, '1AVB': 100.0, '...</td>\n      <td>1</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2997</th>\n      <td>17394</td>\n      <td>{'LMI': 15.0, 'ASMI': 50.0, 'IRBBB': 100.0, 'A...</td>\n      <td>1</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2998</th>\n      <td>18538</td>\n      <td>{'ASMI': 100.0, 'LAFB': 100.0, 'ABQRS': 0.0, '...</td>\n      <td>1</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2999</th>\n      <td>14808</td>\n      <td>{'IMI': 100.0, 'ISCAL': 100.0, 'PVC': 100.0, '...</td>\n      <td>1</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n<p>3000 rows Ã— 4 columns</p>\n</div>"},"metadata":{}}],"execution_count":4},{"id":"EH1Bu5CoYuaJ","cell_type":"markdown","source":"## Load One Signal and Smoke Test","metadata":{"id":"EH1Bu5CoYuaJ"}},{"id":"tuLAXH2nYuaJ","cell_type":"code","source":"def load_signal_single(path_record):\n    \"\"\"\n    Loads all 12 leads of the ECG signal.\n    Returns: numpy array of shape (time_steps, 12)\n    \"\"\"\n    try:\n        recp = os.path.join(DATASET_PATH, path_record)\n        # wfdb.rdsamp returns (signal, fields). We only need signal.\n        sig, fields = wfdb.rdsamp(recp)\n        return sig.astype(np.float32)\n    except:\n        return None\n\ndef smoke_test(df, count=1):\n    print(f\"Showing first {count} records (12-Lead view)...\")\n\n    lead_names = ['I', 'II', 'III', 'AVR', 'AVL', 'AVF', 'V1', 'V2', 'V3', 'V4', 'V5', 'V6']\n\n    for row in df.itertuples():\n        sig = load_signal_single(row.filename_hr)\n        if sig is None:\n            continue\n\n        # Ensure length (now handles 12 leads)\n        sig = ensure_len(sig, SAMPLE_RATE * 10)\n\n        # Plot all 12 leads stacked\n        fig, axes = plt.subplots(12, 1, figsize=(10, 15), sharex=True)\n\n        for i in range(12):\n            ax = axes[i]\n            ax.plot(sig[:, i], color='black', linewidth=0.8)\n            ax.set_ylabel(lead_names[i], rotation=0, labelpad=20, fontsize=10, fontweight='bold')\n            ax.grid(True, linestyle='--', alpha=0.5)\n            if i < 11:\n                ax.tick_params(labelbottom=False)\n\n        plt.suptitle(f\"Record: {row.filename_hr}\\nBinary: {row.label_binary} | Multi: {row.label_multiclass}\", y=1.02)\n        plt.xlabel(\"Time (samples)\")\n        plt.tight_layout()\n        plt.show()\n\n        count -= 1\n        if count == 0:\n            return\n\nsmoke_test(df=ptb_df)","metadata":{"id":"tuLAXH2nYuaJ","trusted":true,"execution":{"iopub.status.busy":"2026-01-26T12:18:08.676031Z","iopub.execute_input":"2026-01-26T12:18:08.676275Z"}},"outputs":[{"name":"stdout","text":"Showing first 1 records (12-Lead view)...\n","output_type":"stream"}],"execution_count":null},{"id":"cNH48EgoYuaK","cell_type":"markdown","source":"## Build Raw Dataset","metadata":{"id":"cNH48EgoYuaK"}},{"id":"Tm6nRzRd-ZnS","cell_type":"code","source":"def build_raw_dataset(df):\n    X, y_bin, y_multi, rec_ids = [], [], [], []\n\n    for r in tqdm(df.itertuples(), desc=\"Loading ECG signals\"):\n        # Load the full 12-lead signal\n        sig = load_signal_single(r.filename_hr)\n        if sig is None:\n            continue\n\n        # Ensure length is exactly 5000 samples (handles 12 leads automatically)\n        sig = ensure_len(sig, SAMPLE_RATE * 10)\n\n        X.append(sig)\n        y_bin.append(r.label_binary)\n        y_multi.append(r.label_multiclass)\n        rec_ids.append(r.filename_hr)\n\n    # Stack to create 3D Array: (Samples, Time, Leads)\n    X = np.stack(X)\n    y_bin = np.array(y_bin)\n    y_multi = np.array(y_multi)\n\n    return X, y_bin, y_multi, rec_ids\n\n# --- Execute the Build ---\nX, y_bin, y_multi, rec_ids = build_raw_dataset(ptb_df)\n\n# --- Normalize ---\nX = normalize_per_record(X)\n\nprint(\"\\nDataset Built!\")\nprint(\"X shape:\", X.shape)\nprint(\"y_bin shape:\", y_bin.shape)\nprint(\"y_multi shape:\", y_multi.shape)","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Tm6nRzRd-ZnS","outputId":"48176de1-a7b9-45c9-9c54-2fb7cf2b300e","trusted":true},"outputs":[],"execution_count":null},{"id":"FCTcjFr4YuaK","cell_type":"markdown","source":"## Feature Extraction","metadata":{"id":"FCTcjFr4YuaK"}},{"id":"OgN19ARjYuaK","cell_type":"code","source":"WINDOW_LEN = 300\nPRE_R = 100\nPOST_R = 200\nSAFETY_MARGIN = 10\n\nN_PCA_COMPONENTS = 5\n\n\ndef get_median_beats(sig):\n    \"\"\"\n    Extracts the median beat from a 12-lead signal.\n    Input: sig (5000, 12)\n    Output: median_beat (12, 300) -> Transposed for PCA consistency\n    \"\"\"\n    # 1. Detect Peaks on Lead II (Index 1)\n    peaks, _ = find_peaks(sig[:, 1], distance=150, height=0.1)\n\n    # If no peaks, return zeros (12 leads x 300 samples)\n    if len(peaks) == 0:\n        return np.zeros((12, WINDOW_LEN), dtype=np.float32)\n\n    beats = []\n\n    for i, r_curr in enumerate(peaks):\n        # --- A. Define Ideal Territory ---\n        ideal_start = r_curr - PRE_R\n        ideal_end = r_curr + POST_R\n\n        # --- B. Define Safety Limits ---\n        if i > 0:\n            limit_start = peaks[i-1] + POST_R + SAFETY_MARGIN\n        else:\n            limit_start = 0\n\n        if i < len(peaks) - 1:\n            limit_end = peaks[i+1] - PRE_R - SAFETY_MARGIN\n        else:\n            limit_end = sig.shape[0]\n\n        # --- C. Truncate ---\n        actual_start = max(ideal_start, limit_start)\n        actual_end = min(ideal_end, limit_end)\n\n        actual_start = max(0, actual_start)\n        actual_end = min(sig.shape[0], actual_end)\n\n        # --- D. The Canvas Method ---\n        canvas = np.zeros((WINDOW_LEN, 12), dtype=np.float32)\n        canvas_start_idx = PRE_R - (r_curr - actual_start)\n\n        chunk = sig[actual_start:actual_end]\n        chunk_len = len(chunk)\n\n        if chunk_len == 0 or canvas_start_idx < 0 or (canvas_start_idx + chunk_len) > WINDOW_LEN:\n            continue\n\n        canvas[canvas_start_idx : canvas_start_idx + chunk_len] = chunk\n        beats.append(canvas)\n\n    # --- E. Compute Median ---\n    if len(beats) == 0:\n        return np.zeros((12, WINDOW_LEN), dtype=np.float32)\n\n    beats = np.stack(beats) # Shape: (N_beats, 300, 12)\n    median_beat = np.median(beats, axis=0) # Shape: (300, 12)\n\n    # Transpose to return (12, 300)\n    return median_beat.T\n\ndef train_pca_model(X):\n    \"\"\"\n    Trains PCA on training set.\n    Input: X (N, 5000, 12)\n    Output: Trained PCA object\n    \"\"\"\n    print(\"Preparing data for PCA training...\")\n    all_beats = []\n\n    for x in tqdm(X, desc=\"Extracting Training Beats\"):\n        mb = get_median_beats(x) # Returns (12, 300)\n        all_beats.append(mb)     # List of (12, 300) arrays\n\n    # Stack: (N_patients * 12, 300)\n    X_for_pca = np.vstack(all_beats)\n\n    print(f\"Training PCA on {X_for_pca.shape[0]} individual lead waveforms...\")\n    pca = PCA(n_components=N_PCA_COMPONENTS)\n    pca.fit(X_for_pca)\n\n    print(f\"PCA Variance Explained: {np.sum(pca.explained_variance_ratio_):.2%}\")\n    return pca\n\ndef extract_pca_features(X, pca_model):\n    \"\"\"\n    Transforms signals into features.\n    Input: X (N, 5000, 12)\n    Output: features (N, 60)\n    \"\"\"\n    features = []\n\n    for x in tqdm(X, desc=\"Transforming Features\"):\n        mb = get_median_beats(x) # (12, 300)\n\n        # Project: Input (12, 300) -> Output (12, 5)\n        coeffs = pca_model.transform(mb)\n\n        # Flatten: 12 * 5 = 60 features\n        features.append(coeffs.flatten())\n\n    return np.array(features)","metadata":{"id":"OgN19ARjYuaK","trusted":true},"outputs":[],"execution_count":null},{"id":"xR1b0QMwYuaK","cell_type":"markdown","source":"## Train Test Split and Features Extraction","metadata":{"id":"xR1b0QMwYuaK"}},{"id":"0xQ1q4S6YuaK","cell_type":"code","source":"# 1. Stratified Split (Raw Data)\nX_train, X_test, y_train_bin, y_test_bin, y_train_multi, y_test_multi = train_test_split(\n    X,\n    y_bin,\n    y_multi,\n    test_size=TEST_SIZE,\n    stratify=y_multi,\n    random_state=SEED\n)\n\nprint(f\"Split Sizes: Train={len(X_train)}, Test={len(X_test)}\")\n\n# 2. Train PCA (On Training Set Only)\npca = train_pca_model(X_train)\n\n# 3. Extract Features\nprint(\"\\nExtracting features for Training Set...\")\nXf_train = extract_pca_features(X_train, pca)\n\nprint(\"\\nExtracting features for Test Set...\")\nXf_test = extract_pca_features(X_test, pca)\n\n# 4. Scale Features\nscaler = StandardScaler()\nXf_train = scaler.fit_transform(Xf_train)\nXf_test = scaler.transform(Xf_test)","metadata":{"id":"0xQ1q4S6YuaK","trusted":true},"outputs":[],"execution_count":null},{"id":"N3ra2ieaAqhz","cell_type":"markdown","source":"## Features Display","metadata":{"id":"N3ra2ieaAqhz"}},{"id":"PGDKcsfnAte3","cell_type":"code","source":"# --- Flexible Feature Display Function ---\ndef show_feature_table(features, lead_idx=None):\n    \"\"\"\n    Creates a DataFrame for the features and displays the head.\n    lead_idx: Integer (0-11) to filter for a specific lead.\n              If None, displays all 60 features.\n    \"\"\"\n    # 1. Generate Column Names\n    lead_names = ['I', 'II', 'III', 'AVR', 'AVL', 'AVF', 'V1', 'V2', 'V3', 'V4', 'V5', 'V6']\n    col_names = [f\"{lead}_C{i+1}\" for lead in lead_names for i in range(N_PCA_COMPONENTS)]\n\n    # 2. Create Full DataFrame\n    df = pd.DataFrame(features, columns=col_names)\n\n    # 3. Filter or Show All\n    if lead_idx is not None:\n        if 0 <= lead_idx < 12:\n            target_lead = lead_names[lead_idx]\n            # Select columns containing the lead name (e.g., \"II_C1\", \"II_C2\"...)\n            # We use specific slicing math: start = idx*5, end = start+5\n            start_col = lead_idx * N_PCA_COMPONENTS\n            end_col = start_col + N_PCA_COMPONENTS\n\n            print(f\"\\n--- Features for Lead {target_lead} (Index {lead_idx}) ---\")\n            display(df.iloc[:, start_col:end_col].head())\n        else:\n            print(f\"Error: Lead index must be 0-11.\")\n    else:\n        print(\"\\n--- All 60 Features (12 Leads x 5 Components) ---\")\n        display(df.head())\n\n# --- Usage Examples ---\n# Example 1: Show Lead II (Index 1) - Often the most important lead\nshow_feature_table(Xf_train, lead_idx=1)\n\n# Example 2: Show All Features\n# show_feature_table(Xf_train)","metadata":{"id":"PGDKcsfnAte3","trusted":true},"outputs":[],"execution_count":null},{"id":"9Z_mctMpA37S","cell_type":"markdown","source":"## Universal Evaluator","metadata":{"id":"9Z_mctMpA37S"}},{"id":"gaW5-cuCBEBv","cell_type":"code","source":"def evaluate_model(model_name, y_true, y_pred):\n    \"\"\"\n    Calculates metrics and displays confusion matrix.\n    Returns a dictionary of metrics.\n    \"\"\"\n    print(f\"\\n{'='*40}\")\n    print(f\"EVALUATION: {model_name}\")\n    print(f\"{'='*40}\")\n\n    # 1. Calculate Metrics\n    acc = accuracy_score(y_true, y_pred)\n    prec = precision_score(y_true, y_pred, zero_division=0)\n    rec = recall_score(y_true, y_pred, zero_division=0)\n    f1 = f1_score(y_true, y_pred, zero_division=0)\n\n    # 2. Print Report\n    print(f\"Accuracy:  {acc:.4f}\")\n    print(f\"Precision: {prec:.4f}\")\n    print(f\"Recall:    {rec:.4f}\")\n    print(f\"F1 Score:  {f1:.4f}\")\n    print(\"-\" * 20)\n    print(\"Classification Report:\")\n    print(classification_report(y_true, y_pred, target_names=['Normal', 'Abnormal']))\n\n    # 3. Plot Confusion Matrix\n    cm = confusion_matrix(y_true, y_pred)\n    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=['Normal', 'Abnormal'])\n\n    fig, ax = plt.subplots(figsize=(5, 5))\n    disp.plot(cmap='Blues', ax=ax, colorbar=False)\n    ax.set_title(f\"{model_name} Confusion Matrix\")\n    plt.show()\n\n    # 4. Return Object\n    return {\n        \"model\": model_name,\n        \"accuracy\": acc,\n        \"precision\": prec,\n        \"recall\": rec,\n        \"f1 score\": f1\n    }\n","metadata":{"id":"gaW5-cuCBEBv","trusted":true},"outputs":[],"execution_count":null},{"id":"GD4DLoAzYuaK","cell_type":"markdown","source":"## Machine Learning Models Training","metadata":{"id":"GD4DLoAzYuaK"}},{"id":"X6EAxSr_YuaK","cell_type":"code","source":"# Train Logistic Regression Model\nlr_model = LogisticRegression(max_iter=1000, random_state=SEED)\nprint(\"Training Logistic Regression on PCA Features...\")\nlr_model.fit(Xf_train, y_train_bin)\n\n# Train Random Forest Model\nrf_model = RandomForestClassifier(n_estimators=200, random_state=SEED)\nprint(\"Training Random Forest on PCA Features...\")\nrf_model.fit(Xf_train, y_train_bin)","metadata":{"id":"X6EAxSr_YuaK","trusted":true},"outputs":[],"execution_count":null},{"id":"8bRhfMMgYuaK","cell_type":"markdown","source":"## Machine Learning Models Evaluation","metadata":{"id":"8bRhfMMgYuaK"}},{"id":"eGMcqjDtYuaK","cell_type":"code","source":"# Train Logistic Regression Model\ny_pred_lr = lr_model.predict(Xf_test)\nres_lr = evaluate_model(\"Logistic Regression (PCA)\", y_test_bin, y_pred_lr)\njoblib.dump(lr_model, os.path.join(OUTPUT_DIR, 'lr_model.pkl'))\n\n# Train Random Forest Model\ny_pred_rf = rf_model.predict(Xf_test)\nres_rf = evaluate_model(\"Random Forest (PCA)\", y_test_bin, y_pred_rf)\njoblib.dump(rf_model, os.path.join(OUTPUT_DIR, 'rf_model.pkl'))","metadata":{"id":"eGMcqjDtYuaK","trusted":true},"outputs":[],"execution_count":null},{"id":"JqK2SFFUYuaK","cell_type":"markdown","source":"## CNN Model Building and Training (Binary Classification)","metadata":{"id":"JqK2SFFUYuaK"}},{"id":"ScjmD1UpYuaK","cell_type":"code","source":"# 1. Define CNN Architecture\ndef build_cnn(input_shape):\n    model = Sequential()\n    model.add(Input(shape=input_shape))\n\n    # --- Block 1: The \"Spike Detector\" (Low-level features) ---\n    model.add(Conv1D(filters=32, kernel_size=10, padding='same'))\n    model.add(BatchNormalization())\n    model.add(Activation('relu'))\n    model.add(MaxPooling1D(pool_size=2))\n\n    # --- Block 2: The \"Wave Detector\" (Mid-level features) ---\n    model.add(Conv1D(filters=64, kernel_size=10, padding='same'))\n    model.add(BatchNormalization())\n    model.add(Activation('relu'))\n    model.add(MaxPooling1D(pool_size=2))\n\n    # --- Block 3: The \"Complex Shape Detector\" (High-level features) ---\n    # This layer sees the relationship between P, QRS, and T\n    model.add(Conv1D(filters=128, kernel_size=10, padding='same'))\n    model.add(BatchNormalization())\n    model.add(Activation('relu'))\n    model.add(MaxPooling1D(pool_size=2))\n\n    # --- Block 4: Optional Deep Feature Extraction ---\n    model.add(Conv1D(filters=256, kernel_size=10, padding='same'))\n    model.add(BatchNormalization())\n    model.add(Activation('relu'))\n    # Global Average Pooling replaces Flatten (Reduces parameters, prevents overfitting)\n    model.add(GlobalAveragePooling1D())\n\n    # --- Classifier Head ---\n    model.add(Dense(128, activation='relu'))\n    model.add(Dropout(0.5)) # Standard dropout to force robustness\n    model.add(Dense(64, activation='relu'))\n    model.add(Dropout(0.3))\n\n    model.add(Dense(1, activation='sigmoid')) # Binary Output\n\n    # Compile\n    optimizer = Adam(learning_rate=0.001)\n    model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy'])\n    return model\n\n# 3. Build\nprint(f\"\\n{'='*40}\")\nprint(\"TRAINING CNN\")\nprint(f\"{'='*40}\")\n\ncnn = build_cnn((5000, 12))\n\ncallbacks_cnn = [\n    EarlyStopping(monitor='val_loss', patience=8, restore_best_weights=True, verbose=1),\n    ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=3, min_lr=0.00001, verbose=1)\n]\n\n# 4. Train\nhistory_cnn = cnn.fit(\n    X_train, y_train_bin,\n    validation_data=(X_test, y_test_bin),\n    epochs=CNN_EPOCHS,\n    batch_size=BATCH_SIZE,\n    callbacks=callbacks_cnn,\n    verbose=1\n)\n\n#5. Save\ncnn.save(os.path.join(OUTPUT_DIR, 'cnn_model_raw.h5'))","metadata":{"id":"ScjmD1UpYuaK","trusted":true},"outputs":[],"execution_count":null},{"id":"2fLHl6yGDSEd","cell_type":"markdown","source":"##  CNN Model Evaluation (Binary Classification)","metadata":{"id":"2fLHl6yGDSEd"}},{"id":"06Dkk02LDXRQ","cell_type":"code","source":"# Predict\nprint(f\"\\n{'='*50}\")\nprint(\"CNN EVALUATION\")\nprint(f\"{'='*50}\")\n\nprint(\"\\nGenerating Predictions...\")\ny_pred_prob_cnn = cnn.predict(X_test)\ny_pred_bin_cnn = (y_pred_prob_cnn > 0.5).astype(int).flatten()\n\n# Evaluate\nprint(\"\\nFinal Evaluation Results:\")\nres_cnn = evaluate_model(\"CNN (Raw)\", y_test_bin, y_pred_bin_cnn)","metadata":{"id":"06Dkk02LDXRQ","trusted":true},"outputs":[],"execution_count":null},{"id":"c38afd07-1b90-4d64-ae0a-f5edb3303b63","cell_type":"markdown","source":"## AG-ResNET Model Building and Training (Binary Classification)","metadata":{}},{"id":"586fc853-1136-4bf5-a8ec-b9953f29403f","cell_type":"code","source":"# ==============================================================================\n# 1. MODEL CONFIGURATION\n# ==============================================================================\nINPUT_SHAPE = (5000, 12)\nNUM_CLASSES = 1\nPOOL_SIZE = 2\nNUM_BLOCKS = 4\nFILTERS_START = 64\n\nLEARNING_RATE = 0.0005 \nBATCH_SIZE = 16\nEPOCHS = 50\n\nIDX_SEPTAL = [6, 7]\nIDX_ANTERIOR = [8, 9]\nIDX_LATERAL = [0, 4, 10, 11]\nIDX_INFERIOR = [1, 2, 5]\nIDX_POSTERIOR = [3]\n\n# ==============================================================================\n# 2. HELPER BLOCKS (VAE CLASSES REMOVED)\n# ==============================================================================\n\ndef residual_block(x, filters, kernel_size=7, stride=1):\n    \"\"\"Creates a standard Residual Block.\"\"\"\n\n    shortcut = x\n\n    # Path A\n    x = Conv1D(filters, kernel_size, strides=stride, padding='same')(x)\n    x = BatchNormalization()(x)\n    x = Activation('swish')(x)\n    x = SpatialDropout1D(0.2)(x)\n\n    x = Conv1D(filters, kernel_size, strides=1, padding='same')(x)\n    x = BatchNormalization()(x)\n\n    # Path B\n    if x.shape[-1] != shortcut.shape[-1] or stride != 1:\n        shortcut = Conv1D(filters, 1, strides=stride, padding='same')(shortcut)\n        shortcut = BatchNormalization()(shortcut)\n\n    x = Add()([x, shortcut])\n    x = Activation('swish')(x)\n    return x\n\n# ==============================================================================\n# 3. ARCHITECTURE BUILDER (PURE CLASSIFIER)\n# ==============================================================================\n\ndef build_ag_resnet(input_shape=INPUT_SHAPE):\n\n    # --- BLOCK 1: INPUT & TRIAGE ---\n    input_layer = Input(shape=input_shape, name=\"ecg_input\")\n\n    septal = Lambda(lambda x: tf.gather(x, IDX_SEPTAL, axis=2), name=\"Septal\")(input_layer)\n    anterior = Lambda(lambda x: tf.gather(x, IDX_ANTERIOR, axis=2), name=\"Anterior\")(input_layer)\n    lateral = Lambda(lambda x: tf.gather(x, IDX_LATERAL, axis=2), name=\"Lateral\")(input_layer)\n    inferior = Lambda(lambda x: tf.gather(x, IDX_INFERIOR, axis=2), name=\"Inferior\")(input_layer)\n    posterior = Lambda(lambda x: tf.gather(x, IDX_POSTERIOR, axis=2), name=\"Posterior\")(input_layer)\n\n    # --- BLOCK 2: ENCODER ---\n    branches = []\n    for region_name, branch_in in zip([\"Sept\", \"Ant\", \"Lat\", \"Inf\", \"Post\"],\n                                      [septal, anterior, lateral, inferior, posterior]):\n\n        # Initial Feature Extraction\n        x = Conv1D(FILTERS_START, 7, strides=POOL_SIZE, padding='same', name=f\"{region_name}_Conv1\")(branch_in)\n        x = BatchNormalization()(x)\n        x = Activation('swish')(x)\n\n        # Dynamic Residual Stacking\n        current_filters = FILTERS_START * 2\n        for i in range(NUM_BLOCKS):\n            x = residual_block(x, current_filters, stride=POOL_SIZE)\n            current_filters *= 2\n\n        x = GlobalAveragePooling1D(name=f\"{region_name}_GlobalPool\")(x)\n        branches.append(x)\n\n    # --- BLOCK 3: FUSION (No VAE Sampling) ---\n    merged = Concatenate(name=\"Anatomical_Fusion\")(branches)\n\n    # --- BLOCK 4: CLASSIFICATION HEAD ---\n    c = BatchNormalization()(merged)\n    c = Dense(128, activation='swish')(c)\n    c = Dropout(0.3)(c)\n    \n    classification_output = Dense(NUM_CLASSES, activation='sigmoid', name=\"classification\")(c)\n\n    # --- BLOCK 5: COMPILATION (Pure Classification) ---\n    model = Model(inputs=input_layer, outputs=classification_output, name=\"AG_ResNet\")\n\n    # Added clipnorm=1.0 to prevent gradient explosions\n    model.compile(\n        optimizer=Adam(learning_rate=LEARNING_RATE, clipnorm=1.0), \n        loss=\"binary_crossentropy\",\n        metrics=[\"accuracy\"]\n    )\n\n    return model\n\n# ==============================================================================\n# 4. EXECUTION PIPELINE (SIMPLIFIED)\n# ==============================================================================\n\nprint(f\"{'='*50}\")\nprint(f\"AG-ResNet TRAINING (PURE CLASSIFIER)\")\nprint(f\"Input: {INPUT_SHAPE}\")\nprint(f\"Structure: {NUM_BLOCKS} Blocks | Start Filters: {FILTERS_START}\")\nprint(f\"{'='*50}\")\n\n# Build\nmodel_ag_resnet = build_ag_resnet()\n\n# Train\ncallbacks_ag_resnet = [\n    ReduceLROnPlateau(\n        monitor='val_loss', factor=0.2, \n        patience=5, min_lr=1e-5, verbose=1\n    ),\n    EarlyStopping(\n        monitor='val_loss', patience=12, \n        restore_best_weights=True, verbose=1\n    ),\n    ModelCheckpoint(\n        os.path.join(OUTPUT_DIR, \"best_ag_resnet.keras\"),\n        monitor='val_accuracy', save_best_only=True, verbose=1)\n]\n\n\nweight_healthy = 1.0\nweight_sick = 3.0\n\nprint(\"\\nStarting Training...\")\nhistory_ag_resnet = model_ag_resnet.fit(\n    x=X_train,\n    y=y_train_bin,\n    validation_data=(X_test, y_test_bin),\n    epochs=EPOCHS,\n    batch_size=BATCH_SIZE,\n    callbacks=callbacks_ag_resnet,\n    class_weight={0: weight_healthy, 1: weight_sick},\n    verbose=1\n)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"0cd75470-440f-48c6-9d1e-c9dc530c04bb","cell_type":"markdown","source":"## AG-ResNET Model Evaluation (Binary Classification)","metadata":{}},{"id":"dc24dae0-d9ef-4d6c-9acd-529be2d6db6e","cell_type":"code","source":"# Predict\nprint(f\"\\n{'='*50}\")\nprint(\"AG-ResNet EVALUATION\")\nprint(f\"{'='*50}\")\n\nprint(\"\\nGenerating Predictions...\")\ny_pred_prob_ag_resnet = model_ag_resnet.predict(X_test).flatten()\ny_pred_bin_ag_resnet = (y_pred_prob_ag_resnet > 0.5).astype(int)\n\n# Evaluate\nprint(\"\\nFinal Evaluation Results:\")\nres_ag_resnet = evaluate_model(\"AG-ResNet (RAW)\", y_test_bin, y_pred_bin_ag_resnet)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"a29b6ef5-e7e1-4d53-a564-9f0490c7107d","cell_type":"markdown","source":"## AG-ResVAE Model Building and Training (Binary Classification) OLD","metadata":{}},{"id":"1c2bc69d-8510-4804-ba08-f88270a85c56","cell_type":"code","source":"# # ==============================================================================\n# # 1. MODEL CONFIGURATION\n# # ==============================================================================\n# INPUT_SHAPE = (5000, 12)\n# NUM_CLASSES = 1\n# POOL_SIZE = 2\n# NUM_BLOCKS = 4\n# FILTERS_START = 64\n# LATENT_DIM = 128\n\n# LEARNING_RATE = 3e-4\n# BATCH_SIZE = 16\n# EPOCHS = 50\n\n# WEIGHT_CLASSIFICATION = 1.0\n# WEIGHT_RECONSTRUCTION = 0.5\n# KL_LOSS_WEIGHT = 0.01\n\n# IDX_SEPTAL = [6, 7]\n# IDX_ANTERIOR = [8, 9]\n# IDX_LATERAL = [0, 4, 10, 11]\n# IDX_INFERIOR = [1, 2, 5]\n# IDX_POSTERIOR = [3]\n\n# # ==============================================================================\n# # 2. HELPER BLOCKS\n# # ==============================================================================\n\n# def residual_block(x, filters, kernel_size=7, stride=1):\n#     \"\"\"Creates a Residual Block.\"\"\"\n\n#     shortcut = x\n\n#     # Path A\n#     x = Conv1D(filters, kernel_size, strides=stride, padding='same')(x)\n#     x = BatchNormalization()(x)\n#     x = Activation('swish')(x)\n#     x = SpatialDropout1D(0.3)(x)\n\n#     x = Conv1D(filters, kernel_size, strides=1, padding='same')(x)\n#     x = BatchNormalization()(x)\n\n#     # Path B\n#     if x.shape[-1] != shortcut.shape[-1] or stride != 1:\n#         shortcut = Conv1D(filters, 1, strides=stride, padding='same')(shortcut)\n#         shortcut = BatchNormalization()(shortcut)\n\n#     x = Add()([x, shortcut])\n#     x = Activation('swish')(x)\n#     return x\n\n# class Sampling(Layer):\n#     \"\"\"VAE Sampling Layer.\"\"\"\n#     def call(self, inputs):\n#         z_mean, z_log_var = inputs\n#         batch = tf.shape(z_mean)[0]\n#         dim = tf.shape(z_mean)[1]\n#         epsilon = tf.keras.backend.random_normal(shape=(batch, dim))\n#         return z_mean + tf.exp(0.5 * z_log_var) * epsilon\n\n# class KLLossLayer(Layer):\n#     \"\"\"Calculates KL Divergence Loss safely.\"\"\"\n#     def call(self, inputs):\n#         z_mean, z_log_var = inputs\n#         kl_loss = -0.5 * tf.reduce_mean(1 + z_log_var - tf.square(z_mean) - tf.exp(z_log_var))\n#         self.add_loss(kl_loss * KL_LOSS_WEIGHT)\n#         return inputs\n\n# # ==============================================================================\n# # 3. ARCHITECTURE BUILDER\n# # ==============================================================================\n\n# def build_ag_resvae(input_shape=INPUT_SHAPE, latent_dim=LATENT_DIM):\n\n#     # --- BLOCK 1: INPUT & TRIAGE ---\n#     input_layer = Input(shape=input_shape, name=\"ecg_input\")\n\n#     septal = Lambda(lambda x: tf.gather(x, IDX_SEPTAL, axis=2), name=\"Septal\")(input_layer)\n#     anterior = Lambda(lambda x: tf.gather(x, IDX_ANTERIOR, axis=2), name=\"Anterior\")(input_layer)\n#     lateral = Lambda(lambda x: tf.gather(x, IDX_LATERAL, axis=2), name=\"Lateral\")(input_layer)\n#     inferior = Lambda(lambda x: tf.gather(x, IDX_INFERIOR, axis=2), name=\"Inferior\")(input_layer)\n#     posterior = Lambda(lambda x: tf.gather(x, IDX_POSTERIOR, axis=2), name=\"Posterior\")(input_layer)\n\n#     # --- BLOCK 2: ENCODER ---\n#     branches = []\n#     for region_name, branch_in in zip([\"Sept\", \"Ant\", \"Lat\", \"Inf\", \"Post\"],\n#                                       [septal, anterior, lateral, inferior, posterior]):\n\n#         # Initial Feature Extraction\n#         x = Conv1D(FILTERS_START, 7, strides=POOL_SIZE, padding='same', name=f\"{region_name}_Conv1\")(branch_in)\n#         x = BatchNormalization()(x)\n#         x = Activation('swish')(x)\n\n#         # Dynamic Residual Stacking\n#         current_filters = FILTERS_START * 2\n#         for i in range(NUM_BLOCKS):\n#             x = residual_block(x, current_filters, stride=POOL_SIZE)\n#             current_filters *= 2\n\n#         x = GlobalAveragePooling1D(name=f\"{region_name}_GlobalPool\")(x)\n#         branches.append(x)\n\n#     # --- BLOCK 3: BOTTLENECK (Latent Vector creation) ---\n#     merged = Concatenate(name=\"Anatomical_Fusion\")(branches)\n\n#     z_mean = Dense(latent_dim, name=\"z_mean\")(merged)\n#     z_log_var = Dense(latent_dim, name=\"z_log_var\")(merged)\n#     _ = KLLossLayer()([z_mean, z_log_var])\n#     z = Sampling(name=\"z_sampling\")([z_mean, z_log_var])\n\n#     # --- BLOCK 4: HEAD A (Diagnosis using Latent Vector) ---\n#     c = Dense(128, activation='swish')(z)\n#     c = Dropout(0.5)(c)\n#     classification_output = Dense(NUM_CLASSES, activation='sigmoid', name=\"classification\")(c)\n\n#     # --- BLOCK 5: HEAD B (Reconstruction using Latent Vector) ---\n#     # Calculate Expected Bottleneck Size (Rounding UP)\n#     total_pools = 1 + NUM_BLOCKS\n#     downsample_factor = POOL_SIZE ** total_pools\n#     bottleneck_len = int(np.ceil(input_shape[0] / downsample_factor))\n\n#     # Expand Latent Vector (z)\n#     d = Dense(bottleneck_len * latent_dim, activation=\"swish\")(z)\n#     d = Reshape((bottleneck_len, latent_dim))(d)\n\n#     # Upsample (Reverse Loop)\n#     filters_reverse = [2* FILTERS_START * (2**i) for i in range(NUM_BLOCKS)]\n#     filters_reverse = filters_reverse[::-1]\n\n#     for f in filters_reverse:\n#         d = Conv1DTranspose(f, 7, strides=POOL_SIZE, padding='same', activation='swish')(d)\n\n#     d = Conv1DTranspose(FILTERS_START, 7, strides=POOL_SIZE, padding='same', activation='swish')(d)\n\n#     # SAFETY CROP\n#     current_len = bottleneck_len * downsample_factor\n#     crop_amount = current_len - input_shape[0]\n\n#     if crop_amount > 0:\n#         d = Cropping1D(cropping=(0, crop_amount), name=\"Safety_Crop\")(d)\n\n#     reconstruction_output = Conv1D(input_shape[1], 7, padding='same', activation='linear', name=\"reconstruction\")(d)\n\n#     # --- BLOCK 6: COMPILATION ---\n#     model = Model(inputs=input_layer, outputs=[classification_output, reconstruction_output], name=\"AG_ResVAE\")\n\n#     model.compile(\n#         optimizer=Adam(learning_rate=LEARNING_RATE),\n#         loss={\n#             \"classification\": \"binary_crossentropy\",\n#             \"reconstruction\": \"mse\"\n#         },\n#         loss_weights={\n#             \"classification\": WEIGHT_CLASSIFICATION,\n#             \"reconstruction\": WEIGHT_RECONSTRUCTION\n#         },\n#         metrics={\"classification\": \"accuracy\"}\n#     )\n\n#     return model\n\n# # ==============================================================================\n# # 4. EXECUTION PIPELINE\n# # ==============================================================================\n\n# print(f\"{'='*50}\")\n# print(f\"AG-ResVAE TRAINING (OPTIMIZED)\")\n# print(f\"Input: {INPUT_SHAPE} | Latent Dim: {LATENT_DIM}\")\n# print(f\"Structure: {NUM_BLOCKS} Blocks | Start Filters: {FILTERS_START}\")\n# print(f\"{'='*50}\")\n\n# # Build\n# model_ag_resvae = build_ag_resvae()\n\n# # Train\n# callbacks_ag_resvae = [\n#     ReduceLROnPlateau(\n#         monitor='val_classification_loss', factor=0.1, \n#         patience=5, min_lr=1e-5, verbose=1, mode='min'\n#     ),\n#     EarlyStopping(\n#         monitor='val_classification_loss', patience=12, \n#         restore_best_weights=True, verbose=1, mode='min'\n#     ),\n#     ModelCheckpoint(\n#         os.path.join(OUTPUT_DIR, \"best_ag_resvae.keras\"), \n#         monitor='val_classification_accuracy', save_best_only=True, \n#         verbose=1, mode='max'\n#     )\n# ]\n\n# print(\"\\nStarting Training...\")\n# history_ag_resvae = model_ag_resvae.fit(\n#     x=X_train,\n#     y={\n#         \"classification\": y_train_bin,\n#         \"reconstruction\": X_train\n#     },\n#     validation_data=(\n#         X_test,\n#         {\"classification\": y_test_bin, \"reconstruction\": X_test}\n#     ),\n#     epochs=EPOCHS,\n#     batch_size=BATCH_SIZE,\n#     callbacks=callbacks_ag_resvae,\n#     verbose=1\n# )","metadata":{"trusted":true,"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"id":"MEXZJCVDEZPF","cell_type":"markdown","source":"## AG-ResVAE Model Building and Training (Binary Classification) NEW","metadata":{"id":"MEXZJCVDEZPF"}},{"id":"qYHp0Sk3KfLY","cell_type":"code","source":"# # ==============================================================================\n# # 1. MODEL CONFIGURATION\n# # ==============================================================================\n# INPUT_SHAPE = (5000, 12)\n# NUM_CLASSES = 1\n# POOL_SIZE = 2\n# NUM_BLOCKS = 4\n# FILTERS_START = 64\n# LATENT_DIM = 128\n\n# LEARNING_RATE = 0.001\n# BATCH_SIZE = 16\n# EPOCHS = 50\n\n# WEIGHT_CLASSIFICATION = 1.0\n# WEIGHT_RECONSTRUCTION = 0.5\n# KL_LOSS_WEIGHT = 0.01\n\n# IDX_SEPTAL = [6, 7]\n# IDX_ANTERIOR = [8, 9]\n# IDX_LATERAL = [0, 4, 10, 11]\n# IDX_INFERIOR = [1, 2, 5]\n# IDX_POSTERIOR = [3]\n\n# # ==============================================================================\n# # 2. HELPER BLOCKS\n# # ==============================================================================\n\n# def residual_block(x, filters, kernel_size=7, stride=1):\n#     \"\"\"Creates a Residual Block.\"\"\"\n\n#     shortcut = x\n\n#     # Path A\n#     x = Conv1D(filters, kernel_size, strides=stride, padding='same')(x)\n#     x = BatchNormalization()(x)\n#     x = Activation('swish')(x)\n#     x = SpatialDropout1D(0.3)(x)\n\n#     x = Conv1D(filters, kernel_size, strides=1, padding='same')(x)\n#     x = BatchNormalization()(x)\n\n#     # Path B\n#     if x.shape[-1] != shortcut.shape[-1] or stride != 1:\n#         shortcut = Conv1D(filters, 1, strides=stride, padding='same')(shortcut)\n#         shortcut = BatchNormalization()(shortcut)\n\n#     x = Add()([x, shortcut])\n#     x = Activation('swish')(x)\n#     return x\n\n# class Sampling(Layer):\n#     \"\"\"VAE Sampling Layer.\"\"\"\n#     def call(self, inputs):\n#         z_mean, z_log_var = inputs\n#         batch = tf.shape(z_mean)[0]\n#         dim = tf.shape(z_mean)[1]\n#         epsilon = tf.keras.backend.random_normal(shape=(batch, dim))\n#         return z_mean + tf.exp(0.5 * z_log_var) * epsilon\n\n# class KLLossLayer(Layer):\n#     \"\"\"Calculates KL Divergence Loss safely.\"\"\"\n#     def call(self, inputs):\n#         z_mean, z_log_var = inputs\n#         kl_loss = -0.5 * tf.reduce_mean(1 + z_log_var - tf.square(z_mean) - tf.exp(z_log_var))\n#         self.add_loss(kl_loss * KL_LOSS_WEIGHT)\n#         return inputs\n\n# # ==============================================================================\n# # 3. ARCHITECTURE BUILDER\n# # ==============================================================================\n\n# def build_ag_resvae(input_shape=INPUT_SHAPE, latent_dim=LATENT_DIM):\n\n#     # --- BLOCK 1: INPUT & TRIAGE ---\n#     input_layer = Input(shape=input_shape, name=\"ecg_input\")\n\n#     septal = Lambda(lambda x: tf.gather(x, IDX_SEPTAL, axis=2), name=\"Septal\")(input_layer)\n#     anterior = Lambda(lambda x: tf.gather(x, IDX_ANTERIOR, axis=2), name=\"Anterior\")(input_layer)\n#     lateral = Lambda(lambda x: tf.gather(x, IDX_LATERAL, axis=2), name=\"Lateral\")(input_layer)\n#     inferior = Lambda(lambda x: tf.gather(x, IDX_INFERIOR, axis=2), name=\"Inferior\")(input_layer)\n#     posterior = Lambda(lambda x: tf.gather(x, IDX_POSTERIOR, axis=2), name=\"Posterior\")(input_layer)\n\n#     # --- BLOCK 2: ENCODER ---\n#     branches = []\n#     for region_name, branch_in in zip([\"Sept\", \"Ant\", \"Lat\", \"Inf\", \"Post\"],\n#                                       [septal, anterior, lateral, inferior, posterior]):\n\n#         # Initial Feature Extraction\n#         x = Conv1D(FILTERS_START, 7, strides=POOL_SIZE, padding='same', name=f\"{region_name}_Conv1\")(branch_in)\n#         x = BatchNormalization()(x)\n#         x = Activation('swish')(x)\n\n#         # Dynamic Residual Stacking\n#         current_filters = FILTERS_START * 2\n#         for i in range(NUM_BLOCKS):\n#             x = residual_block(x, current_filters, stride=POOL_SIZE)\n#             current_filters *= 2\n\n#         x = GlobalAveragePooling1D(name=f\"{region_name}_GlobalPool\")(x)\n#         branches.append(x)\n\n#     # --- BLOCK 3: BOTTLENECK (Latent Vector creation) ---\n#     merged = Concatenate(name=\"Anatomical_Fusion\")(branches)\n\n#     z_mean = Dense(latent_dim, name=\"z_mean\")(merged)\n#     z_log_var = Dense(latent_dim, name=\"z_log_var\")(merged)\n#     _ = KLLossLayer()([z_mean, z_log_var])\n#     z = Sampling(name=\"z_sampling\")([z_mean, z_log_var])\n\n#     # --- BLOCK 4: HEAD A (Diagnosis) ---\n#     c = Dense(256, activation='swish', kernel_initializer='he_normal')(z)\n#     c = BatchNormalization()(c)\n#     c = Dropout(0.4)(c)\n\n#     c = Dense(128, activation='swish', kernel_initializer='he_normal')(z)\n#     c = BatchNormalization()(c)\n#     c = Dropout(0.4)(c)\n\n#     classification_output = Dense(NUM_CLASSES, activation='sigmoid', name=\"classification\")(c)\n\n#     # --- BLOCK 5: HEAD B (Reconstruction using Latent Vector) ---\n#     # Calculate Expected Bottleneck Size (Rounding UP)\n#     total_pools = 1 + NUM_BLOCKS\n#     downsample_factor = POOL_SIZE ** total_pools\n#     bottleneck_len = int(np.ceil(input_shape[0] / downsample_factor))\n\n#     # Expand Latent Vector (z)\n#     d = Dense(bottleneck_len * latent_dim, activation=\"swish\")(z)\n#     d = Reshape((bottleneck_len, latent_dim))(d)\n\n#     # Upsample (Reverse Loop)\n#     filters_reverse = [2* FILTERS_START * (2**i) for i in range(NUM_BLOCKS)]\n#     filters_reverse = filters_reverse[::-1]\n\n#     for f in filters_reverse:\n#         d = Conv1DTranspose(f, 7, strides=POOL_SIZE, padding='same', activation='swish')(d)\n\n#     d = Conv1DTranspose(FILTERS_START, 7, strides=POOL_SIZE, padding='same', activation='swish')(d)\n\n#     # SAFETY CROP\n#     current_len = bottleneck_len * downsample_factor\n#     crop_amount = current_len - input_shape[0]\n\n#     if crop_amount > 0:\n#         d = Cropping1D(cropping=(0, crop_amount), name=\"Safety_Crop\")(d)\n\n#     reconstruction_output = Conv1D(input_shape[1], 7, padding='same', activation='linear', name=\"reconstruction\")(d)\n\n#     # --- BLOCK 6: COMPILATION ---\n#     model = Model(inputs=input_layer, outputs=[classification_output, reconstruction_output], name=\"AG_ResVAE\")\n\n#     model.compile(\n#         optimizer=Adam(learning_rate=LEARNING_RATE),\n#         loss={\n#             \"classification\": \"binary_crossentropy\",\n#             \"reconstruction\": \"mse\"\n#         },\n#         loss_weights={\n#             \"classification\": WEIGHT_CLASSIFICATION,\n#             \"reconstruction\": WEIGHT_RECONSTRUCTION\n#         },\n#         metrics={\"classification\": \"accuracy\"}\n#     )\n\n#     return model\n\n# # ==============================================================================\n# # 4. EXECUTION PIPELINE\n# # ==============================================================================\n\n# print(f\"{'='*50}\")\n# print(f\"AG-ResVAE TRAINING (OPTIMIZED)\")\n# print(f\"Input: {INPUT_SHAPE} | Latent Dim: {LATENT_DIM}\")\n# print(f\"Structure: {NUM_BLOCKS} Blocks | Start Filters: {FILTERS_START}\")\n# print(f\"{'='*50}\")\n\n# # Build\n# model_ag_resvae = build_ag_resvae()\n\n# # Train\n# callbacks_ag_resvae = [\n#     ReduceLROnPlateau(\n#         monitor='val_classification_loss', factor=0.1, \n#         patience=5, min_lr=1e-5, verbose=1, mode='min'\n#     ),\n#     EarlyStopping(\n#         monitor='val_classification_loss', patience=12, \n#         restore_best_weights=True, verbose=1, mode='min'\n#     ),\n#     ModelCheckpoint(\n#         os.path.join(OUTPUT_DIR, \"best_ag_resvae.keras\"), \n#         monitor='val_classification_accuracy', save_best_only=True, \n#         verbose=1, mode='max'\n#     )\n# ]\n\n# # sample_weights_array = np.ones(shape=(len(y_train_bin),))\n# # sample_weights_array[y_train_bin == 1] = 3.0\n# # sample_weights_recon = np.ones(shape=(len(y_train_bin),))\n\n# print(\"\\nStarting Training...\")\n# history_ag = model_ag_resvae.fit(\n#     x=X_train,\n#     y={\n#         \"classification\": y_train_bin,\n#         \"reconstruction\": X_train\n#     },\n#     validation_data=(\n#         X_test,\n#         {\"classification\": y_test_bin, \"reconstruction\": X_test}\n#     ),\n#     epochs=EPOCHS,\n#     batch_size=BATCH_SIZE,\n#     callbacks=callbacks_ag_resvae,\n#     # sample_weight=[sample_weights_array, sample_weights_recon],\n#     verbose=1\n# )\n\n# # history_ag = model_ag_resvae.fit(\n# #     x=X_train,\n# #     y=[y_train_bin, X_train],\n# #     validation_data=(X_test, [y_test_bin, X_test]),\n# #     epochs=EPOCHS,\n# #     batch_size=BATCH_SIZE,\n# #     callbacks=callbacks_ag_resvae,\n# #     sample_weight=[sample_weights_array, None], # <-- LIST\n# #     verbose=1\n# # )","metadata":{"id":"qYHp0Sk3KfLY","trusted":true,"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"id":"0W6xTivHD82C","cell_type":"markdown","source":"## AG-ResVAE Model Evaluation (Binary Classification)","metadata":{"id":"0W6xTivHD82C"}},{"id":"3GTtUXAFD_9z","cell_type":"code","source":"# # Predict\n# print(f\"\\n{'='*50}\")\n# print(\"AG-ResVAE EVALUATION\")\n# print(f\"{'='*50}\")\n\n# print(\"\\nGenerating Predictions...\")\n# all_preds = model_ag_resvae.predict(X_test)\n# y_pred_prob_ag_resvae = all_preds[0]\n# y_pred_bin_ag_resvae = (y_pred_prob_ag_resvae > 0.5).astype(int).flatten()\n\n# # Evaluate\n# print(\"\\nFinal Evaluation Results:\")\n# res_ag_resvae = evaluate_model(\"AG-ResVAE (RAW)\", y_test_bin, y_pred_bin_ag_resvae)","metadata":{"id":"3GTtUXAFD_9z","trusted":true,"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"id":"Y6TcxCuUYuaL","cell_type":"markdown","source":"## Final Result Of All Models","metadata":{"id":"Y6TcxCuUYuaL"}},{"id":"e732549a-a366-46eb-90f1-f68c9fd94203","cell_type":"code","source":"# --- Standard Imports for Loading ---\nimport os\nimport joblib\nfrom keras.models import load_model\n\nprint(f\"{'='*50}\")\nprint(\"FINAL PIPELINE: LOADING MODELS & PREDICTING\")\nprint(f\"{'='*50}\\n\")\n\n# ==========================================\n# 1. Load Machine Learning Models (Scikit-Learn)\n# ==========================================\n\nprint(\"Loading ML Models...\")\nlr_loaded = joblib.load(os.path.join(OUTPUT_DIR, 'lr_model.pkl'))\nrf_loaded = joblib.load(os.path.join(OUTPUT_DIR, 'rf_model.pkl'))\n\n# ==========================================\n# 2. Load Deep Learning Models (Keras)\n# ==========================================\n\nprint(\"Loading DL Models...\")\n# Custom layer 'Sampling' and 'KLLossLayer' need to be passed for AG-ResVAE\ncnn_loaded = load_model(os.path.join(OUTPUT_DIR, 'cnn_model_raw.h5'))\nag_resnet_loaded = load_model(os.path.join(OUTPUT_DIR, 'best_ag_resnet.keras'))\nag_resvae_loaded = load_model(\n    os.path.join(OUTPUT_DIR, 'best_ag_resvae.keras'),\n    custom_objects={'Sampling': Sampling, 'KLLossLayer': KLLossLayer}\n)\n\nprint(\"All models loaded successfully!\\n\")\n\n# ==========================================\n# 3. Generate Predictions on Test Data\n# ==========================================\n\n# ML Predictions (using PCA features)\ny_pred_bin_lr = lr_loaded.predict(Xf_test)\ny_pred_bin_rf = rf_loaded.predict(Xf_test)\n\n# DL Predictions (using Raw signals)\ny_pred_prob_cnn = cnn_loaded.predict(X_test, verbose=0)\ny_pred_bin_cnn = (y_pred_prob_cnn > 0.5).astype(int).flatten()\n\ny_pred_prob_ag_resnet = ag_resnet_loaded.predict(X_test, verbose=0)\ny_pred_bin_ag_resnet = (y_pred_prob_ag_resnet > 0.5).astype(int).flatten()\n\nag_resvae_preds = ag_resvae_loaded.predict(X_test, verbose=0)\ny_pred_prob_ag_resvae = ag_resvae_preds[0] # [0] is classification head\ny_pred_bin_ag_resvae = (y_pred_prob_ag_resvae > 0.5).astype(int).flatten()\n\n# ==========================================\n# 4. Final Evaluation\n# ==========================================\n\nprint(\"\\n--- Final Test Set Evaluations ---\")\nfinal_lr = evaluate_model(\"Loaded Logistic Regression\", y_test_bin, y_pred_bin_lr)\nfinal_rf = evaluate_model(\"Loaded Random Forest\", y_test_bin, y_pred_bin_rf)\nfinal_cnn = evaluate_model(\"Loaded CNN\", y_test_bin, y_pred_bin_cnn)\nfinal_ag_resnet = evaluate_model(\"Loaded AG-ResNET\", y_test_bin, y_pred_bin_ag_resnet)\nfinal_ag_resvae = evaluate_model(\"Loaded AG-ResVAE\", y_test_bin, y_pred_bin_ag_resvae)\n\n# Consolidate results into a final DataFrame\nfinal_results_df = pd.DataFrame([final_lr, final_rf, final_cnn, final_ag_resnet, final_ag_resvae])\ndisplay(final_results_df.style.format({\n    'accuracy': '{:.2%}',\n    'precision': '{:.2%}',\n    'recall': '{:.2%}',\n    'f1 score': '{:.2%}'\n}))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"ca4943de","cell_type":"markdown","source":"# ðŸ“Œ Conclusion\n\nWe trained and compared **three models** for MI detection:\n\n### âœ” Logistic Regression (very fast, uses simple features)  \n### âœ” Random Forest (better than LR in many cases)  \n### âœ” 1D CNN (uses raw ECG, learns automatically)\n\nEvaluation metrics included:\n- Accuracy  \n- Precision  \n- Recall  \n- F1  \n- Confusion Matrix  ","metadata":{"id":"ca4943de"}}]}